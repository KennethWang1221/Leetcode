{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE) Loss Function\n",
    " \n",
    "The **Mean Squared Error (MSE)** measures the average squared difference between actual target values $(y_i)$ and predicted values $(\\hat{y}_i)$:\n",
    " \n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \n",
    "$$\n",
    " \n",
    "### Deriving the Gradient of MSE with Respect to $ \\mathbf{w} $ \n",
    "1. **Expressing MSE in Matrix Form:**\n",
    " \n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} (\\mathbf{y} - \\mathbf{\\hat{y}})^\\top (\\mathbf{y} - \\mathbf{\\hat{y}})\n",
    "$$\n",
    "\n",
    "Substituting $\\mathbf{\\hat{y}} = \\mathbf{Xw}$:\n",
    " \n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} (\\mathbf{y} - \\mathbf{Xw})^\\top (\\mathbf{y} - \\mathbf{Xw})\n",
    "$$\n",
    "\n",
    "1. **Differentiation Step-by-Step:**\n",
    "\n",
    "- **Expand the Expression:**\n",
    " \n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\left[\\mathbf{y}^\\top \\mathbf{y} - \\mathbf{y}^\\top \\mathbf{Xw} - \\mathbf{w}^\\top \\mathbf{X}^\\top \\mathbf{y} + \\mathbf{w}^\\top \\mathbf{X}^\\top \\mathbf{Xw}\\right]\n",
    "$$\n",
    "\n",
    "- **Differentiating Each Term:**\n",
    "\n",
    "- $\\frac{\\partial}{\\partial \\mathbf{w}}(\\mathbf{y}^\\top \\mathbf{y}) = 0$\n",
    "- $\\frac{\\partial}{\\partial \\mathbf{w}}(-2\\mathbf{w}^\\top \\mathbf{X}^\\top \\mathbf{y}) = -2\\mathbf{X}^\\top \\mathbf{y}$\n",
    "- $\\frac{\\partial}{\\partial \\mathbf{w}}(\\mathbf{w}^\\top \\mathbf{X}^\\top \\mathbf{Xw}) = 2\\mathbf{X}^\\top \\mathbf{Xw}$\n",
    "\n",
    "- **Combine the Derivatives:**\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}}\\text{MSE} = \\frac{1}{n} \\left[0 - 2\\mathbf{X}^\\top \\mathbf{y} + 2\\mathbf{X}^\\top \\mathbf{Xw}\\right]\n",
    "$$\n",
    " \n",
    "Simplifies to:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}}\\text{MSE} = \\frac{2}{n} \\mathbf{X}^\\top (\\mathbf{Xw} - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "### Implementation in Code\n",
    "\n",
    "```python\n",
    "def compute_gradients(X, y_true, y_pred):\n",
    "  errors = y_pred - y_true\n",
    "  gradients = np.dot(X.T, errors) * (2 / len(y_true))\n",
    "  return gradients\n",
    " ```\n",
    "\n",
    "The gradient of the MSE loss function with respect to the weight vector $ \\mathbf{w} $\n",
    "\n",
    "$ \\mathbf{w} $ indicates the direction and magnitude by which $ \\mathbf{w} $\n",
    "\n",
    "should be adjusted to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Gradient Descent:\n",
      "Iteration 100/1000, Loss: 0.0323\n",
      "Iteration 200/1000, Loss: 0.0177\n",
      "Iteration 300/1000, Loss: 0.0097\n",
      "Iteration 400/1000, Loss: 0.0053\n",
      "Iteration 500/1000, Loss: 0.0029\n",
      "Iteration 600/1000, Loss: 0.0016\n",
      "Iteration 700/1000, Loss: 0.0009\n",
      "Iteration 800/1000, Loss: 0.0005\n",
      "Iteration 900/1000, Loss: 0.0003\n",
      "Iteration 1000/1000, Loss: 0.0001\n",
      "Final Weights: [1.01003164 0.97050576]\n",
      "Final MSE Loss: 0.00014615956788017066\n",
      "--------------------------------------------------\n",
      "Stochastic Gradient Descent:\n",
      "Iteration 100/1000, Loss: 0.0049\n",
      "Iteration 200/1000, Loss: 0.0004\n",
      "Iteration 300/1000, Loss: 0.0000\n",
      "Iteration 400/1000, Loss: 0.0000\n",
      "Iteration 500/1000, Loss: 0.0000\n",
      "Iteration 600/1000, Loss: 0.0000\n",
      "Iteration 700/1000, Loss: 0.0000\n",
      "Iteration 800/1000, Loss: 0.0000\n",
      "Iteration 900/1000, Loss: 0.0000\n",
      "Iteration 1000/1000, Loss: 0.0000\n",
      "Final Weights: [1.00000058 0.99999813]\n",
      "Final MSE Loss: 6.501074837144217e-13\n",
      "--------------------------------------------------\n",
      "Mini-Batch Gradient Descent:\n",
      "Iteration 100/1000, Loss: 0.0181\n",
      "Iteration 200/1000, Loss: 0.0054\n",
      "Iteration 300/1000, Loss: 0.0016\n",
      "Iteration 400/1000, Loss: 0.0005\n",
      "Iteration 500/1000, Loss: 0.0001\n",
      "Iteration 600/1000, Loss: 0.0000\n",
      "Iteration 700/1000, Loss: 0.0000\n",
      "Iteration 800/1000, Loss: 0.0000\n",
      "Iteration 900/1000, Loss: 0.0000\n",
      "Iteration 1000/1000, Loss: 0.0000\n",
      "Final Weights: [1.00045157 0.99865639]\n",
      "Final MSE Loss: 3.0822816847448454e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error loss.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Actual target values.\n",
    "    - y_pred: Predicted target values.\n",
    "\n",
    "    Returns:\n",
    "    - mse: Mean Squared Error.\n",
    "    \"\"\"\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    return mse\n",
    "\n",
    "def compute_gradients(X, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the MSE loss with respect to the weights.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature matrix.\n",
    "    - y_true: Actual target values.\n",
    "    - y_pred: Predicted target values.\n",
    "\n",
    "    Returns:\n",
    "    - gradients: Gradient of the loss with respect to weights.\n",
    "    \"\"\"\n",
    "    errors = y_pred - y_true\n",
    "    gradients = np.dot(X.T, errors) * (2 / len(y_true))\n",
    "    return gradients\n",
    "\n",
    "def gradient_descent(X, y, weights, learning_rate=0.01, n_iterations=1000, method='batch', batch_size=32):\n",
    "    \"\"\"\n",
    "    Perform Gradient Descent optimization with different variants.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Feature matrix.\n",
    "    - y: Target vector.\n",
    "    - weights: Initial weights.\n",
    "    - learning_rate: Learning rate for weight updates.\n",
    "    - n_iterations: Number of iterations.\n",
    "    - method: Type of Gradient Descent ('stochastic', 'batch', 'mini_batch').\n",
    "    - batch_size: Size of mini-batches for 'mini_batch' variant.\n",
    "\n",
    "    Returns:\n",
    "    - weights: Optimized weights after training.\n",
    "    - history: Loss history over iterations.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    history = []\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        if method == 'batch':\n",
    "            # Batch Gradient Descent: Update weights using the entire dataset\n",
    "            y_pred = np.dot(X, weights)\n",
    "            loss = compute_mse_loss(y, y_pred)\n",
    "            gradients = compute_gradients(X, y, y_pred)\n",
    "            weights -= learning_rate * gradients\n",
    "            history.append(loss)\n",
    "\n",
    "        elif method == 'stochastic':\n",
    "            # Stochastic Gradient Descent: Update weights for each sample\n",
    "            loss = 0.0  # Initialize as float\n",
    "            for i in range(n_samples):\n",
    "                xi = X[i].reshape(1, -1)\n",
    "                yi = y[i]\n",
    "                y_pred = np.dot(xi, weights)[0]  # Extract scalar\n",
    "                loss += (yi - y_pred) ** 2\n",
    "                gradient = compute_gradients(xi, np.array([yi]), y_pred)\n",
    "                weights -= learning_rate * gradient.flatten()\n",
    "            mse = loss / n_samples\n",
    "            history.append(mse)\n",
    "\n",
    "        elif method == 'mini_batch':\n",
    "            # Mini-Batch Gradient Descent: Update weights using mini-batches\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            loss = 0.0  # Initialize as float\n",
    "\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "                y_pred = np.dot(X_batch, weights)\n",
    "                loss += compute_mse_loss(y_batch, y_pred)\n",
    "                gradients = compute_gradients(X_batch, y_batch, y_pred)\n",
    "                weights -= learning_rate * gradients\n",
    "            mse = loss / (n_samples / batch_size)\n",
    "            history.append(mse)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'stochastic', 'batch', or 'mini_batch'.\")\n",
    "\n",
    "        # (Optional) Print loss every 100 iterations\n",
    "        if (iteration + 1) % 100 == 0:\n",
    "            print(f\"Iteration {iteration + 1}/{n_iterations}, Loss: {history[-1]:.4f}\")\n",
    "\n",
    "    return weights, history\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
    "y = np.array([2, 3, 4, 5])\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "n_iterations = 1000\n",
    "batch_size = 2\n",
    "\n",
    "# Initialize weights\n",
    "initial_weights = np.zeros(X.shape[1])\n",
    "\n",
    "# Test Batch Gradient Descent\n",
    "print(\"Batch Gradient Descent:\")\n",
    "final_weights_batch, loss_history_batch = gradient_descent(\n",
    "\tX, y, initial_weights.copy(),\n",
    "\tlearning_rate=learning_rate,\n",
    "\tn_iterations=n_iterations,\n",
    "\tmethod='batch'\n",
    ")\n",
    "print(\"Final Weights:\", final_weights_batch)\n",
    "print(\"Final MSE Loss:\", loss_history_batch[-1])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test Stochastic Gradient Descent\n",
    "print(\"Stochastic Gradient Descent:\")\n",
    "final_weights_sgd, loss_history_sgd = gradient_descent(\n",
    "\tX, y, initial_weights.copy(),\n",
    "\tlearning_rate=learning_rate,\n",
    "\tn_iterations=n_iterations,\n",
    "\tmethod='stochastic'\n",
    ")\n",
    "print(\"Final Weights:\", final_weights_sgd)\n",
    "print(\"Final MSE Loss:\", loss_history_sgd[-1])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test Mini-Batch Gradient Descent\n",
    "print(\"Mini-Batch Gradient Descent:\")\n",
    "final_weights_mbgd, loss_history_mbgd = gradient_descent(\n",
    "\tX, y, initial_weights.copy(),\n",
    "\tlearning_rate=learning_rate,\n",
    "\tn_iterations=n_iterations,\n",
    "\tmethod='mini_batch',\n",
    "\tbatch_size=batch_size\n",
    ")\n",
    "print(\"Final Weights:\", final_weights_mbgd)\n",
    "print(\"Final MSE Loss:\", loss_history_mbgd[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leetcode_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
