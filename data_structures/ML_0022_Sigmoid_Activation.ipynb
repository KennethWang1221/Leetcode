{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Binary Cross-Entropy (BCE)](#2-binary-cross-entropy-bce)\n",
    "   - [2.1. What is Binary Cross-Entropy?](#21-what-is-binary-cross-entropy)\n",
    "   - [2.2. Mathematical Formulation](#22-mathematical-formulation)\n",
    "   - [2.3. Gradient Derivation](#23-gradient-derivation)\n",
    "     - [2.3.1. Derivative of the Sigmoid Function](#231-derivative-of-the-sigmoid-function)\n",
    "     - [2.3.2. Gradient with Respect to Logit $ z $](#232-gradient-with-respect-to-logit-z)\n",
    "     - [2.3.3. Gradient with Respect to Weights $ \\mathbf{w} $ and Bias $ b $](#233-gradient-with-respect-to-weights-w-and-b)\n",
    "3. [Categorical Cross-Entropy (CCE)](#3-categorical-cross-entropy-cce)\n",
    "   - [3.1. What is Categorical Cross-Entropy?](#31-what-is-categorical-cross-entropy)\n",
    "   - [3.2. Mathematical Formulation](#32-mathematical-formulation)\n",
    "   - [3.3. Gradient Derivation](#33-gradient-derivation)\n",
    "     - [3.3.1. Derivative of the Softmax Function](#331-derivative-of-the-softmax-function)\n",
    "     - [3.3.2. Gradient with Respect to Logits $ z_{i,k} $](#332-gradient-with-respect-to-logits-zik)\n",
    "     - [3.3.3. Gradient with Respect to Weights $ \\mathbf{W} $ and Bias $ \\mathbf{b} $](#333-gradient-with-respect-to-weights-w-and-b)\n",
    "4. [Activation Functions](#4-activation-functions)\n",
    "   - [4.1. Sigmoid Function](#41-sigmoid-function)\n",
    "   - [4.2. Softmax Function](#42-softmax-function)\n",
    "5. [Why Pair Sigmoid with BCE and Softmax with CCE?](#5-why-pair-sigmoid-with-bce-and-softmax-with-cce)\n",
    "6. [Practical Examples](#6-practical-examples)\n",
    "   - [6.1. BCE Example: Binary Classification](#61-bce-example-binary-classification)\n",
    "   - [6.2. CCE Example: Multi-Class Classification](#62-cce-example-multi-class-classification)\n",
    "7. [Conclusion](#7-conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In machine learning, particularly in classification tasks, **loss functions** play a pivotal role in guiding the training of models. Among these, **Binary Cross-Entropy (BCE)** and **Categorical Cross-Entropy (CCE)** are two fundamental loss functions used for different types of classification problems. Understanding their formulations, associated **activation functions** (including the detailed derivations of the **sigmoid** and **softmax** functions), and gradient computations is essential for effectively training models like logistic regression and neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Binary Cross-Entropy (BCE)\n",
    "\n",
    "### 2.1. What is Binary Cross-Entropy?\n",
    "\n",
    "**Binary Cross-Entropy (BCE)**, also known as **Log Loss**, is a loss function used for **binary classification** tasks. In these tasks, each instance is categorized into one of two classes, often labeled as 0 or 1. BCE measures the discrepancy between the true labels and the predicted probabilities output by the model.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Binary Classification:** Suited for problems with two distinct classes.\n",
    "- **Probability Output:** Models typically output a probability representing the likelihood of the instance belonging to the positive class.\n",
    "\n",
    "### 2.2. Mathematical Formulation\n",
    "\n",
    "#### **Single Sample BCE Loss**\n",
    "\n",
    "For a single training instance, the BCE loss is defined as:\n",
    "\n",
    "$$\n",
    "L = -\\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y \\in \\{0, 1\\} $ is the **true label**.\n",
    "- $ \\hat{y} \\in (0, 1) $ is the **predicted probability** of the instance belonging to the positive class.\n",
    "\n",
    "#### **Average BCE Loss Over $ n $ Samples**\n",
    "\n",
    "For a dataset containing $ n $ samples, the average BCE loss is:\n",
    "\n",
    "$$\n",
    "\\text{BCE}_{\\text{avg}} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "**Understanding the Components:**\n",
    "- **First Term ($ y_i \\cdot \\log(\\hat{y}_i) $):** Penalizes the model when it predicts a low probability for the true positive class.\n",
    "- **Second Term ($ (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) $):** Penalizes the model when it predicts a high probability for the false positive class.\n",
    "\n",
    "### 2.3. Gradient Derivation\n",
    "\n",
    "To train a model using gradient-based optimization (like Gradient Descent), we need to compute the gradient of the loss with respect to the model parameters. Let's derive the gradients step by step.\n",
    "\n",
    "#### 2.3.1. Derivative of the Sigmoid Function\n",
    "\n",
    "Before diving into the gradient of the BCE loss, it's essential to understand the derivative of the **sigmoid** activation function, as it plays a critical role in the gradient computation.\n",
    "\n",
    "##### **Sigmoid Function Definition**\n",
    "\n",
    "The sigmoid function maps any real-valued number into the (0, 1) interval:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "##### **Derivative of the Sigmoid Function**\n",
    "\n",
    "To compute the gradient of the BCE loss, we'll need the derivative of the sigmoid function with respect to its input $ z $.\n",
    "\n",
    "**Step-by-Step Derivation:**\n",
    "\n",
    "1. **Express the Sigmoid Function:**\n",
    "\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "2. **Differentiate $ \\sigma(z) $ with Respect to $ z $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{d\\sigma(z)}{dz} = \\frac{d}{dz} \\left( \\frac{1}{1 + e^{-z}} \\right )\n",
    "   $$\n",
    "\n",
    "3. **Apply the Chain Rule:**\n",
    "\n",
    "   Let $ u = 1 + e^{-z} $, so $ \\sigma(z) = \\frac{1}{u} $.\n",
    "\n",
    "   $$\n",
    "   \\frac{d\\sigma(z)}{dz} = \\frac{d\\sigma(z)}{du} \\cdot \\frac{du}{dz}\n",
    "   $$\n",
    "\n",
    "4. **Compute Each Derivative:**\n",
    "\n",
    "   - **Derivative of $ \\sigma(z) $ with respect to $ u $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{d\\sigma(z)}{du} = -\\frac{1}{u^2}\n",
    "     $$\n",
    "\n",
    "   - **Derivative of $ u $ with respect to $ z $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{du}{dz} = \\frac{d}{dz} \\left( 1 + e^{-z} \\right ) = -e^{-z}\n",
    "     $$\n",
    "\n",
    "5. **Combine Using the Chain Rule:**\n",
    "\n",
    "   $$\n",
    "   \\frac{d\\sigma(z)}{dz} = -\\frac{1}{u^2} \\cdot (-e^{-z}) = \\frac{e^{-z}}{(1 + e^{-z})^2}\n",
    "   $$\n",
    "\n",
    "6. **Express in Terms of $ \\sigma(z) $:**\n",
    "\n",
    "   Notice that:\n",
    "\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}} \\quad \\text{and} \\quad 1 - \\sigma(z) = \\frac{e^{-z}}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "   Therefore:\n",
    "\n",
    "   $$\n",
    "   \\frac{d\\sigma(z)}{dz} = \\sigma(z) \\cdot (1 - \\sigma(z))\n",
    "   $$\n",
    "\n",
    "**Final Expression:**\n",
    "\n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- The derivative $ \\sigma'(z) $ represents how sensitive the sigmoid function's output is to changes in the input $ z $.\n",
    "- **Properties:**\n",
    "  - The derivative is maximum at $ z = 0 $, where $ \\sigma(z) = 0.5 $, resulting in $ \\sigma'(0) = 0.25 $.\n",
    "  - As $ z $ moves away from 0 (either positively or negatively), $ \\sigma'(z) $ decreases, leading to the **vanishing gradient** problem in deep networks.\n",
    "\n",
    "#### 2.3.2. Gradient with Respect to Logit $ z $\n",
    "\n",
    "**Objective:** Compute $ \\frac{\\partial L}{\\partial z} $, where $ z $ is the logit (the input to the activation function).\n",
    "\n",
    "**Model Setup:**\n",
    "- **Logistic Regression Model:**\n",
    "\n",
    "  $$\n",
    "  z = \\mathbf{x}^\\top \\mathbf{w} + b\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $ \\mathbf{x} $ is the input feature vector.\n",
    "  - $ \\mathbf{w} $ is the weight vector.\n",
    "  - $ b $ is the bias term.\n",
    "\n",
    "- **Sigmoid Activation Function:**\n",
    "\n",
    "  $$\n",
    "  \\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  $$\n",
    "\n",
    "**Step-by-Step Derivation:**\n",
    "\n",
    "1. **Define the Loss Function for a Single Sample:**\n",
    "\n",
    "   $$\n",
    "   L = -\\left[ y \\cdot \\log(\\sigma(z)) + (1 - y) \\cdot \\log(1 - \\sigma(z)) \\right]\n",
    "   $$\n",
    "\n",
    "2. **Compute the Derivative of $ L $ with Respect to $ z $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{dL}{dz} = -\\left[ y \\cdot \\frac{1}{\\sigma(z)} \\cdot \\sigma'(z) + (1 - y) \\cdot \\frac{-1}{1 - \\sigma(z)} \\cdot \\sigma'(z) \\right]\n",
    "   $$\n",
    "\n",
    "   **Explanation:**\n",
    "   - **First Term ($ y \\cdot \\log(\\sigma(z)) $):**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial}{\\partial z} \\left( y \\cdot \\log(\\sigma(z)) \\right) = y \\cdot \\frac{1}{\\sigma(z)} \\cdot \\sigma'(z)\n",
    "     $$\n",
    "\n",
    "   - **Second Term ($ (1 - y) \\cdot \\log(1 - \\sigma(z)) $):**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial}{\\partial z} \\left( (1 - y) \\cdot \\log(1 - \\sigma(z)) \\right) = (1 - y) \\cdot \\frac{-1}{1 - \\sigma(z)} \\cdot \\sigma'(z)\n",
    "     $$\n",
    "\n",
    "3. **Substitute the Sigmoid Derivative ($ \\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z)) $) into the Expression:**\n",
    "\n",
    "   $$\n",
    "   \\frac{dL}{dz} = -\\left[ y \\cdot \\frac{\\sigma(z) \\cdot (1 - \\sigma(z))}{\\sigma(z)} + (1 - y) \\cdot \\frac{-\\sigma(z) \\cdot (1 - \\sigma(z))}{1 - \\sigma(z)} \\right]\n",
    "   $$\n",
    "\n",
    "4. **Simplify the Expression:**\n",
    "\n",
    "   - **First Term Simplification:**\n",
    "\n",
    "     $$\n",
    "     y \\cdot \\frac{\\sigma(z) \\cdot (1 - \\sigma(z))}{\\sigma(z)} = y \\cdot (1 - \\sigma(z))\n",
    "     $$\n",
    "\n",
    "   - **Second Term Simplification:**\n",
    "\n",
    "     $$\n",
    "     (1 - y) \\cdot \\frac{-\\sigma(z) \\cdot (1 - \\sigma(z))}{1 - \\sigma(z)} = -(1 - y) \\cdot \\sigma(z)\n",
    "     $$\n",
    "\n",
    "   - **Combine Both Terms:**\n",
    "\n",
    "     $$\n",
    "     \\frac{dL}{dz} = -\\left[ y \\cdot (1 - \\sigma(z)) - (1 - y) \\cdot \\sigma(z) \\right]\n",
    "     $$\n",
    "\n",
    "5. **Factor Out $ \\sigma(z) $ and Rearrange Terms:**\n",
    "\n",
    "   $$\n",
    "   \\frac{dL}{dz} = \\sigma(z) - y\n",
    "   $$\n",
    "\n",
    "   **Final Gradient Expression:**\n",
    "\n",
    "   $$\n",
    "   \\frac{dL}{dz} = \\hat{y} - y\n",
    "   $$\n",
    "\n",
    "   **Interpretation:**\n",
    "   - **If $ \\hat{y} > y $:** The model predicts a higher probability than the true label. The gradient is positive, indicating the need to **decrease** $ z $ to reduce $ \\hat{y} $.\n",
    "   - **If $ \\hat{y} < y $:** The model predicts a lower probability than the true label. The gradient is negative, indicating the need to **increase** $ z $ to boost $ \\hat{y} $.\n",
    "   - **If $ \\hat{y} = y $:** The gradient is zero, indicating no adjustment is needed.\n",
    "\n",
    "#### 2.3.3. Gradient with Respect to Weights $ \\mathbf{w} $ and Bias $ b $\n",
    "\n",
    "**Objective:** Derive the gradients of the BCE loss with respect to the model's weights $ \\mathbf{w} $ and bias $ b $.\n",
    "\n",
    "**Using the Chain Rule:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\mathbf{w}} \\quad \\text{and} \\quad \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\frac{\\partial L}{\\partial z} = \\hat{y} - y $\n",
    "- $ \\frac{\\partial z}{\\partial \\mathbf{w}} = \\mathbf{x} $\n",
    "- $ \\frac{\\partial z}{\\partial b} = 1 $\n",
    "\n",
    "**Step-by-Step Derivation:**\n",
    "\n",
    "1. **Define $ z $ in Terms of $ \\mathbf{w} $ and $ b $:**\n",
    "\n",
    "   $$\n",
    "   z = \\mathbf{x}^\\top \\mathbf{w} + b\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - $ \\mathbf{x} $ is the input feature vector.\n",
    "   - $ \\mathbf{w} $ is the weight vector.\n",
    "   - $ b $ is the bias term.\n",
    "\n",
    "2. **Compute the Partial Derivative of $ z $ with Respect to $ \\mathbf{w} $ and $ b $:**\n",
    "\n",
    "   - **With Respect to $ \\mathbf{w} $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial z}{\\partial \\mathbf{w}} = \\mathbf{x}\n",
    "     $$\n",
    "\n",
    "     **Explanation:** Since $ z $ is a linear combination of weights and inputs, the derivative with respect to each weight $ w_j $ is the corresponding input feature $ x_j $.\n",
    "\n",
    "   - **With Respect to $ b $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial z}{\\partial b} = 1\n",
    "     $$\n",
    "\n",
    "     **Explanation:** The bias term $ b $ directly adds to $ z $, so its derivative is 1.\n",
    "\n",
    "3. **Apply the Chain Rule to Compute the Gradients:**\n",
    "\n",
    "   - **Gradient with Respect to $ \\mathbf{w} $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial \\mathbf{w}} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\mathbf{w}} = (\\hat{y} - y) \\cdot \\mathbf{x}\n",
    "     $$\n",
    "\n",
    "   - **Gradient with Respect to $ b $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b} = (\\hat{y} - y) \\cdot 1 = \\hat{y} - y\n",
    "     $$\n",
    "\n",
    "4. **Gradient Expressions for $ n $ Samples:**\n",
    "\n",
    "   When dealing with a batch of $ n $ samples, the gradients are averaged over all samples to ensure scale-invariant updates.\n",
    "\n",
    "   - **Gradient with Respect to Weights $ \\mathbf{w} $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial \\text{BCE}_{\\text{avg}}}{\\partial \\mathbf{w}} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) \\cdot \\mathbf{x}_i = \\frac{1}{n} \\mathbf{X}^\\top (\\hat{\\mathbf{y}} - \\mathbf{y})\n",
    "     $$\n",
    "\n",
    "     Where:\n",
    "     - $ \\mathbf{X} $ is the input matrix of shape $ [n, d] $.\n",
    "     - $ \\hat{\\mathbf{y}} $ is the predicted probabilities vector of shape $ [n] $.\n",
    "     - $ \\mathbf{y} $ is the true labels vector of shape $ [n] $.\n",
    "\n",
    "   - **Gradient with Respect to Bias $ b $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial \\text{BCE}_{\\text{avg}}}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) = \\frac{1}{n} (\\hat{\\mathbf{y}} - \\mathbf{y})^\\top \\mathbf{1}\n",
    "     $$\n",
    "\n",
    "     Where $ \\mathbf{1} $ is a vector of ones of shape $ [n] $.\n",
    "\n",
    "**Summary for BCE:**\n",
    "- **Gradient with Respect to Logit $ z $:** $ \\frac{dL}{dz} = \\hat{y} - y $\n",
    "- **Gradient with Respect to Weights $ \\mathbf{w} $:** $ \\frac{\\partial L}{\\partial \\mathbf{w}} = (\\hat{y} - y) \\cdot \\mathbf{x} $\n",
    "- **Gradient with Respect to Bias $ b $:** $ \\frac{\\partial L}{\\partial b} = \\hat{y} - y $\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Categorical Cross-Entropy (CCE)\n",
    "\n",
    "### 3.1. What is Categorical Cross-Entropy?\n",
    "\n",
    "**Categorical Cross-Entropy (CCE)** is a loss function used for **multi-class classification** tasks, where each instance is assigned to one of three or more classes. Unlike binary classification, multi-class classification involves more complexity due to the increased number of classes and the mutual exclusivity of class assignments.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Multi-Class Classification:** Suited for problems with three or more distinct classes.\n",
    "- **Probability Distribution Output:** Models output a probability distribution over all possible classes.\n",
    "\n",
    "### 3.2. Mathematical Formulation\n",
    "\n",
    "#### **Single Sample CCE Loss**\n",
    "\n",
    "For a single training instance, the CCE loss is defined as:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{k=1}^{C} y_k \\cdot \\log(\\hat{y}_k)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ C $ is the number of classes.\n",
    "- $ y_k \\in \\{0, 1\\} $ is the **true label** in one-hot encoded form (only one $ y_k = 1 $, the rest are 0).\n",
    "- $ \\hat{y}_k \\in (0, 1) $ is the **predicted probability** for class $ k $, obtained via the **softmax** activation function.\n",
    "\n",
    "#### **Average CCE Loss Over $ n $ Samples**\n",
    "\n",
    "For a dataset containing $ n $ samples, the average CCE loss is:\n",
    "\n",
    "$$\n",
    "\\text{CCE}_{\\text{avg}} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{k=1}^{C} y_{i,k} \\cdot \\log(\\hat{y}_{i,k})\n",
    "$$\n",
    "\n",
    "**Understanding the Components:**\n",
    "- **Summation Over Classes:** Ensures that only the probability assigned to the true class contributes to the loss.\n",
    "- **Logarithmic Penalty:** Penalizes the model more severely when the predicted probability for the true class is low.\n",
    "\n",
    "### 3.3. Gradient Derivation\n",
    "\n",
    "To train a multi-class classification model using gradient-based optimization, we need to derive the gradients of the CCE loss with respect to the model parameters. Let's proceed with a meticulous step-by-step derivation.\n",
    "\n",
    "#### 3.3.1. Derivative of the Softmax Function\n",
    "\n",
    "Before diving into the gradient of the CCE loss, it's essential to understand the derivative of the **softmax** activation function, as it plays a critical role in the gradient computation.\n",
    "\n",
    "##### **Softmax Function Definition**\n",
    "\n",
    "The softmax function converts a vector of raw scores (logits) into a probability distribution. For a vector $ \\mathbf{z} = [z_1, z_2, \\ldots, z_C] $, the softmax function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z)_k = \\frac{e^{z_k}}{\\sum_{j=1}^{C} e^{z_j}} \\quad \\text{for } k = 1, 2, \\ldots, C\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ z_k $ is the logit for class $ k $.\n",
    "- $ C $ is the total number of classes.\n",
    "\n",
    "##### **Derivative of the Softmax Function**\n",
    "\n",
    "The derivative of the softmax function is more involved than the sigmoid function due to the interdependence of class probabilities. The derivative is represented by the **Jacobian matrix**, which contains all first-order partial derivatives of the softmax outputs with respect to the inputs.\n",
    "\n",
    "**Step-by-Step Derivation:**\n",
    "\n",
    "1. **Express the Softmax Function:**\n",
    "\n",
    "   $$\n",
    "   \\text{softmax}(z)_k = \\frac{e^{z_k}}{\\sum_{j=1}^{C} e^{z_j}}\n",
    "   $$\n",
    "\n",
    "2. **Differentiate $ \\text{softmax}(z)_k $ with Respect to $ z_i $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{softmax}(z)_k}{\\partial z_i} = \\frac{\\partial}{\\partial z_i} \\left( \\frac{e^{z_k}}{\\sum_{j=1}^{C} e^{z_j}} \\right )\n",
    "   $$\n",
    "\n",
    "3. **Apply the Quotient Rule:**\n",
    "\n",
    "   Let $ u = e^{z_k} $ and $ v = \\sum_{j=1}^{C} e^{z_j} $, so:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{softmax}(z)_k}{\\partial z_i} = \\frac{v \\cdot \\frac{\\partial u}{\\partial z_i} - u \\cdot \\frac{\\partial v}{\\partial z_i}}{v^2}\n",
    "   $$\n",
    "\n",
    "4. **Compute the Partial Derivatives:**\n",
    "\n",
    "   - **For $ i = k $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial u}{\\partial z_i} = \\frac{\\partial e^{z_k}}{\\partial z_k} = e^{z_k} = u\n",
    "     $$\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial v}{\\partial z_i} = \\frac{\\partial}{\\partial z_k} \\left( \\sum_{j=1}^{C} e^{z_j} \\right ) = e^{z_k} = u\n",
    "     $$\n",
    "\n",
    "   - **For $ i \\neq k $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial u}{\\partial z_i} = \\frac{\\partial e^{z_k}}{\\partial z_i} = 0\n",
    "     $$\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial v}{\\partial z_i} = \\frac{\\partial}{\\partial z_i} \\left( \\sum_{j=1}^{C} e^{z_j} \\right ) = e^{z_i}\n",
    "     $$\n",
    "\n",
    "5. **Substitute Back into the Quotient Rule:**\n",
    "\n",
    "   - **For $ i = k $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial \\text{softmax}(z)_k}{\\partial z_k} = \\frac{v \\cdot u - u \\cdot u}{v^2} = \\frac{u(v - u)}{v^2} = \\frac{u}{v} \\cdot \\left(1 - \\frac{u}{v}\\right ) = \\text{softmax}(z)_k \\cdot (1 - \\text{softmax}(z)_k)\n",
    "     $$\n",
    "\n",
    "   - **For $ i \\neq k $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial \\text{softmax}(z)_k}{\\partial z_i} = \\frac{v \\cdot 0 - u \\cdot e^{z_i}}{v^2} = -\\frac{u \\cdot e^{z_i}}{v^2} = -\\frac{e^{z_k}}{v} \\cdot \\frac{e^{z_i}}{v} = -\\text{softmax}(z)_k \\cdot \\text{softmax}(z)_i\n",
    "     $$\n",
    "\n",
    "6. **Combine the Results:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{softmax}(z)_k}{\\partial z_i} = \n",
    "   \\begin{cases}\n",
    "   \\text{softmax}(z)_k \\cdot (1 - \\text{softmax}(z)_k) & \\text{if } i = k \\\\\n",
    "   -\\text{softmax}(z)_k \\cdot \\text{softmax}(z)_i & \\text{if } i \\neq k\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "7. **Expressing the Jacobian Matrix:**\n",
    "\n",
    "   The Jacobian matrix $ J $ for the softmax function is given by:\n",
    "\n",
    "   $$\n",
    "   J_{k,i} = \\frac{\\partial \\text{softmax}(z)_k}{\\partial z_i} = \\text{softmax}(z)_k \\cdot (\\delta_{ki} - \\text{softmax}(z)_i)\n",
    "   $$\n",
    "\n",
    "   Where $ \\delta_{ki} $ is the Kronecker delta, which is 1 if $ k = i $ and 0 otherwise.\n",
    "\n",
    "**Final Expression:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{softmax}(z)_k}{\\partial z_i} = \\text{softmax}(z)_k \\cdot (\\delta_{ki} - \\text{softmax}(z)_i)\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **Diagonal Elements ($ k = i $):** Represent the derivative of the probability of class $ k $ with respect to its own logit $ z_k $. This term is always positive and less than 1.\n",
    "- **Off-Diagonal Elements ($ k \\neq i $):** Represent the derivative of the probability of class $ k $ with respect to the logit of a different class $ z_i $. These terms are always negative, reflecting the mutual exclusivity enforced by softmax.\n",
    "\n",
    "#### 3.3.2. Gradient with Respect to Logits $ z_{i,k} $\n",
    "\n",
    "**Objective:** Compute $ \\frac{\\partial L}{\\partial z_{i,k}} $, where $ z_{i,k} $ is the logit for class $ k $ in sample $ i $.\n",
    "\n",
    "**Model Setup:**\n",
    "- **Softmax Activation Function:**\n",
    "\n",
    "  $$\n",
    "  \\hat{y}_{i,k} = \\text{softmax}(z_{i,k}) = \\frac{e^{z_{i,k}}}{\\sum_{j=1}^{C} e^{z_{i,j}}}\n",
    "  $$\n",
    "\n",
    "- **Loss Function:**\n",
    "\n",
    "  $$\n",
    "  L = -\\sum_{k=1}^{C} y_k \\cdot \\log(\\hat{y}_k)\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $ y_k $ is 1 if the true class for sample $ i $ is $ k $, else 0 (one-hot encoding).\n",
    "\n",
    "**Step-by-Step Derivation:**\n",
    "\n",
    "1. **Define the Loss Function for a Single Sample:**\n",
    "\n",
    "   $$\n",
    "   L_i = -\\sum_{k=1}^{C} y_{i,k} \\cdot \\log(\\hat{y}_{i,k})\n",
    "   $$\n",
    "\n",
    "2. **Compute the Derivative of $ L_i $ with Respect to $ z_{i,k} $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L_i}{\\partial z_{i,k}} = -\\sum_{j=1}^{C} y_{i,j} \\cdot \\frac{\\partial}{\\partial z_{i,k}} \\log(\\hat{y}_{i,j})\n",
    "   $$\n",
    "\n",
    "   **Explanation:**\n",
    "   - The loss depends on all logits through the softmax function, but due to one-hot encoding, only the term corresponding to the true class contributes directly.\n",
    "\n",
    "3. **Differentiate $ \\log(\\hat{y}_{i,j}) $ with Respect to $ z_{i,k} $:**\n",
    "\n",
    "   - **For $ j = k $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial}{\\partial z_{i,k}} \\log(\\hat{y}_{i,k}) = \\frac{1}{\\hat{y}_{i,k}} \\cdot \\frac{\\partial \\hat{y}_{i,k}}{\\partial z_{i,k}} = \\frac{1}{\\hat{y}_{i,k}} \\cdot \\hat{y}_{i,k} \\cdot (1 - \\hat{y}_{i,k}) = 1 - \\hat{y}_{i,k}\n",
    "     $$\n",
    "\n",
    "   - **For $ j \\neq k $:**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial}{\\partial z_{i,k}} \\log(\\hat{y}_{i,j}) = \\frac{1}{\\hat{y}_{i,j}} \\cdot \\frac{\\partial \\hat{y}_{i,j}}{\\partial z_{i,k}} = \\frac{1}{\\hat{y}_{i,j}} \\cdot (-\\hat{y}_{i,j} \\cdot \\hat{y}_{i,k}) = -\\hat{y}_{i,k}\n",
    "     $$\n",
    "\n",
    "4. **Substitute Back into the Loss Derivative:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L_i}{\\partial z_{i,k}} = -\\left[ y_{i,k} \\cdot (1 - \\hat{y}_{i,k}) + \\sum_{j \\neq k} y_{i,j} \\cdot (-\\hat{y}_{i,k}) \\right]\n",
    "   $$\n",
    "\n",
    "   **Simplification:**\n",
    "   - Due to one-hot encoding, $ y_{i,j} = 0 $ for $ j \\neq k $, so the summation term vanishes.\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L_i}{\\partial z_{i,k}} = -\\left[ y_{i,k} \\cdot (1 - \\hat{y}_{i,k}) \\right] = \\hat{y}_{i,k} - y_{i,k}\n",
    "   $$\n",
    "\n",
    "   **Final Gradient Expression:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L_i}{\\partial z_{i,k}} = \\hat{y}_{i,k} - y_{i,k}\n",
    "   $$\n",
    "\n",
    "   **Interpretation:**\n",
    "   - **If $ \\hat{y}_{i,k} > y_{i,k} $:** The model assigns a higher probability to class $ k $ than the true label. The gradient is positive, indicating the need to **decrease** $ z_{i,k} $.\n",
    "   - **If $ \\hat{y}_{i,k} < y_{i,k} $:** The model assigns a lower probability to class $ k $ than the true label. The gradient is negative, indicating the need to **increase** $ z_{i,k} $.\n",
    "   - **If $ \\hat{y}_{i,k} = y_{i,k} $:** The gradient is zero, indicating no adjustment is needed.\n",
    "\n",
    "#### 3.3.3. Gradient with Respect to Weights $ \\mathbf{W} $ and Bias $ \\mathbf{b} $\n",
    "\n",
    "**Objective:** Derive the gradients of the CCE loss with respect to the model's weight matrix $ \\mathbf{W} $ and bias vector $ \\mathbf{b} $.\n",
    "\n",
    "**Using the Chain Rule:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{z}} \\cdot \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}} \\quad \\text{and} \\quad \\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{\\partial L}{\\partial \\mathbf{z}} \\cdot \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{b}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\frac{\\partial L}{\\partial \\mathbf{z}} $ is the matrix of gradients with respect to each logit $ z_{i,k} $.\n",
    "\n",
    "**Step-by-Step Derivation:**\n",
    "\n",
    "1. **Define the Logit Matrix:**\n",
    "\n",
    "   $$\n",
    "   \\mathbf{Z} = \\mathbf{X} \\mathbf{W} + \\mathbf{b}\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - $ \\mathbf{X} \\in \\mathbb{R}^{n \\times d} $ is the input feature matrix.\n",
    "   - $ \\mathbf{W} \\in \\mathbb{R}^{d \\times C} $ is the weight matrix.\n",
    "   - $ \\mathbf{b} \\in \\mathbb{R}^{C} $ is the bias vector.\n",
    "\n",
    "2. **Compute the Gradient of the Loss with Respect to the Logit Matrix $ \\mathbf{Z} $:**\n",
    "\n",
    "   From the previous section:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L_i}{\\partial z_{i,k}} = \\hat{y}_{i,k} - y_{i,k}\n",
    "   $$\n",
    "\n",
    "   This can be represented in matrix form as:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{CCE}_{\\text{avg}}}{\\partial \\mathbf{Z}} = \\frac{1}{n} (\\hat{\\mathbf{Y}} - \\mathbf{Y})\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - $ \\hat{\\mathbf{Y}} \\in \\mathbb{R}^{n \\times C} $ is the matrix of predicted probabilities.\n",
    "   - $ \\mathbf{Y} \\in \\mathbb{R}^{n \\times C} $ is the matrix of true labels.\n",
    "\n",
    "3. **Compute the Gradient with Respect to Weights $ \\mathbf{W} $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}} = \\mathbf{X}\n",
    "   $$\n",
    "\n",
    "   **Explanation:**\n",
    "   - Each element $ z_{i,k} = \\mathbf{x}_i^\\top \\mathbf{w}_k + b_k $.\n",
    "   - Differentiating $ z_{i,k} $ with respect to $ \\mathbf{w}_k $ yields $ \\mathbf{x}_i $.\n",
    "\n",
    "   Therefore, the gradient with respect to $ \\mathbf{W} $ is:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{CCE}_{\\text{avg}}}{\\partial \\mathbf{W}} = \\mathbf{X}^\\top \\cdot \\frac{\\partial \\text{CCE}_{\\text{avg}}}{\\partial \\mathbf{Z}} = \\frac{1}{n} \\mathbf{X}^\\top (\\hat{\\mathbf{Y}} - \\mathbf{Y})\n",
    "   $$\n",
    "\n",
    "4. **Compute the Gradient with Respect to Bias $ \\mathbf{b} $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}} = \\mathbf{1}\n",
    "   $$\n",
    "\n",
    "   Where $ \\mathbf{1} $ is a vector of ones of shape $ [n] $.\n",
    "\n",
    "   Therefore, the gradient with respect to $ \\mathbf{b} $ is:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{CCE}_{\\text{avg}}}{\\partial \\mathbf{b}} = \\frac{\\partial \\text{CCE}_{\\text{avg}}}{\\partial \\mathbf{Z}} \\cdot \\mathbf{1} = \\frac{1}{n} (\\hat{\\mathbf{Y}} - \\mathbf{Y})^\\top \\mathbf{1}\n",
    "   $$\n",
    "\n",
    "   **Interpretation:**\n",
    "   - Each element of the bias gradient is the average of $ \\hat{y}_{i,k} - y_{i,k} $ across all samples $ i $ for class $ k $.\n",
    "\n",
    "**Summary for CCE:**\n",
    "- **Gradient with Respect to Logit $ z_{i,k} $:** $ \\frac{\\partial L_i}{\\partial z_{i,k}} = \\hat{y}_{i,k} - y_{i,k} $\n",
    "- **Gradient with Respect to Weights $ \\mathbf{W} $:** $ \\frac{\\partial \\text{CCE}_{\\text{avg}}}{\\partial \\mathbf{W}} = \\frac{1}{n} \\mathbf{X}^\\top (\\hat{\\mathbf{Y}} - \\mathbf{Y}) $\n",
    "- **Gradient with Respect to Bias $ \\mathbf{b} $:** $ \\frac{\\partial \\text{CCE}_{\\text{avg}}}{\\partial \\mathbf{b}} = \\frac{1}{n} (\\hat{\\mathbf{Y}} - \\mathbf{Y})^\\top \\mathbf{1} $\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns. As established, **Sigmoid** and **Softmax** are the go-to activation functions for BCE and CCE, respectively. Let's revisit their definitions and properties to reinforce understanding.\n",
    "\n",
    "### 4.1. Sigmoid Function\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "**Properties:**\n",
    "- **Range:** (0, 1)\n",
    "- **Monotonic:** Always increasing.\n",
    "- **Derivative:**\n",
    "\n",
    "  $$\n",
    "  \\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))\n",
    "  $$\n",
    "\n",
    "**Usage:**\n",
    "- **Binary Classification:** Provides a single probability output representing the likelihood of the positive class.\n",
    "\n",
    "**Advantages:**\n",
    "- **Probabilistic Interpretation:** Outputs can be directly interpreted as probabilities.\n",
    "- **Smooth Gradient:** Facilitates gradient-based optimization.\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Vanishing Gradient Problem:** Gradients can become very small for large positive or negative inputs, slowing down learning.\n",
    "- **Output Not Zero-Centered:** Can cause issues in optimization dynamics.\n",
    "\n",
    "### 4.2. Softmax Function\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z)_k = \\frac{e^{z_k}}{\\sum_{j=1}^{C} e^{z_j}} \\quad \\text{for } k = 1, 2, \\ldots, C\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ z_k $ is the logit for class $ k $.\n",
    "- $ C $ is the total number of classes.\n",
    "\n",
    "**Properties:**\n",
    "- **Range:** (0, 1) for each class $ k $.\n",
    "- **Sum to One:** $ \\sum_{k=1}^{C} \\text{softmax}(z)_k = 1 $.\n",
    "- **Derivative:**\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial \\text{softmax}(z)_k}{\\partial z_i} = \\text{softmax}(z)_k \\cdot (\\delta_{ki} - \\text{softmax}(z)_i)\n",
    "  $$\n",
    "\n",
    "  Where $ \\delta_{ki} $ is the Kronecker delta, which is 1 if $ k = i $ and 0 otherwise.\n",
    "\n",
    "**Usage:**\n",
    "- **Multi-Class Classification:** Provides a probability distribution over multiple classes, ensuring that the sum of probabilities across classes is 1.\n",
    "\n",
    "**Advantages:**\n",
    "- **Probability Distribution:** Ensures outputs form a valid probability distribution.\n",
    "- **Mutual Exclusivity:** Suitable for tasks where each instance belongs to only one class.\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Computationally Intensive:** Requires computing exponentials for all classes.\n",
    "- **Sensitive to Input Scaling:** Large input values can cause numerical instability.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Why Pair Sigmoid with BCE and Softmax with CCE?\n",
    "\n",
    "The pairing of activation functions with loss functions is not arbitrary; it's driven by the mathematical properties and requirements of the classification tasks.\n",
    "\n",
    "### 5.1. BCE with Sigmoid\n",
    "\n",
    "**Reasoning:**\n",
    "\n",
    "- **Single Probability Output:** Binary classification requires predicting the probability of an instance belonging to one of two classes. The sigmoid function naturally outputs a single probability value in the (0, 1) range.\n",
    "  \n",
    "- **Loss Function Alignment:** BCE is designed to measure the discrepancy between true binary labels and predicted probabilities. The sigmoid activation provides probabilities that BCE expects.\n",
    "  \n",
    "- **Gradient Compatibility:** The gradient derivation for BCE with sigmoid results in simple and effective updates ($ \\hat{y} - y $), facilitating efficient learning.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "- **Task Compatibility:** Perfectly aligns with binary classification needs.\n",
    "- **Mathematical Alignment:** Ensures loss and gradient computations are coherent and effective for optimization.\n",
    "\n",
    "### 5.2. CCE with Softmax\n",
    "\n",
    "**Reasoning:**\n",
    "\n",
    "- **Probability Distribution Over Classes:** Multi-class classification involves predicting the probability distribution across multiple classes. Softmax converts logits into a probability distribution where all probabilities sum to one.\n",
    "  \n",
    "- **Mutual Exclusivity:** In multi-class classification, classes are mutually exclusive; an instance can belong to only one class. Softmax inherently enforces this exclusivity by distributing the probability mass among classes.\n",
    "  \n",
    "- **Loss Function Alignment:** CCE computes the loss based on the probability assigned to the true class, making softmax's distribution an excellent fit.\n",
    "\n",
    "- **Gradient Compatibility:** The gradient derivation for CCE with softmax results in $ \\hat{y}_k - y_k $ for each class $ k $, which effectively guides the model to adjust logits to better match true labels.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "- **Task Compatibility:** Ideal for multi-class classification scenarios.\n",
    "- **Mathematical Alignment:** Ensures loss and gradient computations are coherent and effective for optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Practical Examples\n",
    "\n",
    "To solidify the understanding of BCE and CCE, let's walk through concrete examples demonstrating their application and gradient computations.\n",
    "\n",
    "### 6.1. BCE Example: Binary Classification\n",
    "\n",
    "**Scenario:** Email Spam Detection (Spam vs. Not Spam)\n",
    "\n",
    "**Objective:** Train a logistic regression model to classify emails as spam (1) or not spam (0) based on input features.\n",
    "\n",
    "**Model Setup:**\n",
    "- **Input Features ($ \\mathbf{x} $):** Vector representing email characteristics (e.g., word counts, presence of certain keywords).\n",
    "- **Weights ($ \\mathbf{w} $):** Weight vector to be learned.\n",
    "- **Bias ($ b $):** Bias term to be learned.\n",
    "- **Logit ($ z $):**\n",
    "\n",
    "  $$\n",
    "  z = \\mathbf{x}^\\top \\mathbf{w} + b\n",
    "  $$\n",
    "\n",
    "- **Activation Function:** Sigmoid, producing $ \\hat{y} = \\sigma(z) $.\n",
    "- **Loss Function:** BCE, $ L = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})] $.\n",
    "\n",
    "**Sample Computation:**\n",
    "\n",
    "1. **Given:**\n",
    "   - **True Label:** $ y = 1 $ (Spam)\n",
    "   - **Input Features:** $ \\mathbf{x} = [2, 3] $\n",
    "   - **Initial Weights:** $ \\mathbf{w} = [0.5, -0.5] $\n",
    "   - **Bias:** $ b = 0 $\n",
    "   - **Learning Rate ($ \\eta $):** 0.1\n",
    "\n",
    "2. **Compute Logit ($ z $):**\n",
    "\n",
    "   $$\n",
    "   z = \\mathbf{x}^\\top \\mathbf{w} + b = (2 \\times 0.5) + (3 \\times -0.5) + 0 = 1 - 1.5 = -0.5\n",
    "   $$\n",
    "\n",
    "3. **Compute Predicted Probability ($ \\hat{y} $):**\n",
    "\n",
    "   $$\n",
    "   \\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-(-0.5)}} = \\frac{1}{1 + e^{0.5}} \\approx 0.3775\n",
    "   $$\n",
    "\n",
    "4. **Compute BCE Loss:**\n",
    "\n",
    "   $$\n",
    "   L = -\\left[ 1 \\cdot \\log(0.3775) + 0 \\cdot \\log(1 - 0.3775) \\right] = -\\log(0.3775) \\approx 0.974\n",
    "   $$\n",
    "\n",
    "5. **Compute Gradient with Respect to $ z $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{dL}{dz} = \\hat{y} - y = 0.3775 - 1 = -0.6225\n",
    "   $$\n",
    "\n",
    "6. **Compute Gradient with Respect to Weights $ \\mathbf{w} $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{w}} = (\\hat{y} - y) \\cdot \\mathbf{x} = (-0.6225) \\cdot [2, 3] = [-1.245, -1.8675]\n",
    "   $$\n",
    "\n",
    "7. **Compute Gradient with Respect to Bias $ b $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial b} = \\hat{y} - y = -0.6225\n",
    "   $$\n",
    "\n",
    "8. **Update Weights and Bias Using Gradient Descent:**\n",
    "\n",
    "   - **Weights Update:**\n",
    "\n",
    "     $$\n",
    "     \\mathbf{w}_{\\text{new}} = \\mathbf{w} - \\eta \\cdot \\frac{\\partial L}{\\partial \\mathbf{w}} = [0.5, -0.5] - 0.1 \\cdot [-1.245, -1.8675] = [0.5 + 0.1245, -0.5 + 0.18675] = [0.6245, -0.31325]\n",
    "     $$\n",
    "\n",
    "   - **Bias Update:**\n",
    "\n",
    "     $$\n",
    "     b_{\\text{new}} = b - \\eta \\cdot \\frac{\\partial L}{\\partial b} = 0 - 0.1 \\cdot (-0.6225) = 0 + 0.06225 = 0.06225\n",
    "     $$\n",
    "\n",
    "**Interpretation:**\n",
    "- The weights have been adjusted to increase the predicted probability $ \\hat{y} $ closer to the true label $ y = 1 $, thereby reducing the loss in subsequent iterations.\n",
    "\n",
    "### 6.2. CCE Example: Multi-Class Classification\n",
    "\n",
    "**Scenario:** Handwritten Digit Recognition (Digits 0-3 for Simplicity)\n",
    "\n",
    "**Objective:** Train a neural network to classify images of handwritten digits into one of four classes (0, 1, 2, 3).\n",
    "\n",
    "**Model Setup:**\n",
    "- **Input Features ($ \\mathbf{x} $):** Vector representing image pixel intensities.\n",
    "- **Weight Matrix ($ \\mathbf{W} $):** Matrix to be learned, with one column per class.\n",
    "- **Bias Vector ($ \\mathbf{b} $):** Vector to be learned, with one element per class.\n",
    "- **Logit Matrix ($ \\mathbf{Z} $):**\n",
    "\n",
    "  $$\n",
    "  \\mathbf{Z} = \\mathbf{X} \\mathbf{W} + \\mathbf{b}\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $ \\mathbf{X} \\in \\mathbb{R}^{n \\times d} $ is the input feature matrix.\n",
    "  - $ \\mathbf{W} \\in \\mathbb{R}^{d \\times C} $ is the weight matrix.\n",
    "  - $ \\mathbf{b} \\in \\mathbb{R}^{C} $ is the bias vector.\n",
    "\n",
    "- **Activation Function:** Softmax, producing $ \\hat{\\mathbf{Y}} = \\text{softmax}(\\mathbf{Z}) $.\n",
    "- **Loss Function:** CCE, $ L = -\\sum_{k=1}^{C} y_k \\cdot \\log(\\hat{y}_k) $.\n",
    "\n",
    "**Sample Computation:**\n",
    "\n",
    "1. **Given:**\n",
    "   - **True Label:** Digit 3 (One-hot encoded as $ \\mathbf{y} = [0, 0, 0, 1] $)\n",
    "   - **Input Features:** $ \\mathbf{x} = [2, 1, 3] $ (for simplicity)\n",
    "   - **Initial Weight Matrix ($ \\mathbf{W} $):**\n",
    "\n",
    "     $$\n",
    "     \\mathbf{W} = \\begin{bmatrix}\n",
    "     0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
    "     0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
    "     0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "   - **Initial Bias Vector ($ \\mathbf{b} $):** $ [0, 0, 0, 0] $\n",
    "   - **Learning Rate ($ \\eta $):** 0.1\n",
    "\n",
    "2. **Compute Logit Vector ($ \\mathbf{z} $):**\n",
    "\n",
    "   $$\n",
    "   \\mathbf{z} = \\mathbf{x}^\\top \\mathbf{W} + \\mathbf{b} = [2 \\times 0.1 + 1 \\times 0.5 + 3 \\times 0.9, \\, 2 \\times 0.2 + 1 \\times 0.6 + 3 \\times 1.0, \\, 2 \\times 0.3 + 1 \\times 0.7 + 3 \\times 1.1, \\, 2 \\times 0.4 + 1 \\times 0.8 + 3 \\times 1.2]\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\mathbf{z} = [0.2 + 0.5 + 2.7, \\, 0.4 + 0.6 + 3.0, \\, 0.6 + 0.7 + 3.3, \\, 0.8 + 0.8 + 3.6] = [3.4, 4.0, 4.6, 5.2]\n",
    "   $$\n",
    "\n",
    "3. **Apply Softmax Activation to Compute $ \\hat{\\mathbf{Y}} $:**\n",
    "\n",
    "   $$\n",
    "   \\hat{y}_k = \\frac{e^{z_k}}{\\sum_{j=1}^{4} e^{z_j}} \\quad \\text{for } k = 1, 2, 3, 4\n",
    "   $$\n",
    "\n",
    "   - **Compute Exponentials:**\n",
    "\n",
    "     $$\n",
    "     e^{z} = [e^{3.4}, e^{4.0}, e^{4.6}, e^{5.2}] \\approx [29.9641, 54.5982, 99.4843, 181.2725]\n",
    "     $$\n",
    "\n",
    "   - **Compute Sum of Exponentials:**\n",
    "\n",
    "     $$\n",
    "     S = 29.9641 + 54.5982 + 99.4843 + 181.2725 \\approx 365.3181\n",
    "     $$\n",
    "\n",
    "   - **Compute Softmax Outputs:**\n",
    "\n",
    "     $$\n",
    "     \\hat{\\mathbf{Y}} = \\left[ \\frac{29.9641}{365.3181}, \\frac{54.5982}{365.3181}, \\frac{99.4843}{365.3181}, \\frac{181.2725}{365.3181} \\right] \\approx [0.0819, 0.1492, 0.2724, 0.4965]\n",
    "     $$\n",
    "\n",
    "4. **Compute CCE Loss:**\n",
    "\n",
    "   $$\n",
    "   L = -\\sum_{k=1}^{4} y_k \\cdot \\log(\\hat{y}_k) = -\\log(\\hat{y}_4) = -\\log(0.4965) \\approx 0.700\n",
    "   $$\n",
    "\n",
    "5. **Compute Gradient with Respect to Logits $ z_k $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial z_k} = \\hat{y}_k - y_k\n",
    "   $$\n",
    "\n",
    "   Specifically:\n",
    "   - **For $ k = 4 $ (true class):**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial z_4} = \\hat{y}_4 - y_4 = 0.4965 - 1 = -0.5035\n",
    "     $$\n",
    "\n",
    "   - **For $ k = 1, 2, 3 $ (other classes):**\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial z_k} = \\hat{y}_k - y_k = \\hat{y}_k - 0 = \\hat{y}_k\n",
    "     $$\n",
    "\n",
    "     So:\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial z_1} = 0.0819, \\quad \\frac{\\partial L}{\\partial z_2} = 0.1492, \\quad \\frac{\\partial L}{\\partial z_3} = 0.2724\n",
    "     $$\n",
    "\n",
    "6. **Compute Gradient with Respect to Weight Matrix $ \\mathbf{W} $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{CCE}_{\\text{avg}}}{\\partial \\mathbf{W}} = \\frac{1}{n} \\mathbf{X}^\\top (\\hat{\\mathbf{Y}} - \\mathbf{Y})\n",
    "   $$\n",
    "\n",
    "   Since $ n = 1 $ in this single-sample example:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{CCE}}{\\partial \\mathbf{W}} = \\mathbf{x}^\\top (\\hat{\\mathbf{Y}} - \\mathbf{Y}) = \\begin{bmatrix} x_1 (\\hat{y}_1 - y_1) & x_1 (\\hat{y}_2 - y_2) & x_1 (\\hat{y}_3 - y_3) & x_1 (\\hat{y}_4 - y_4) \\\\ x_2 (\\hat{y}_1 - y_1) & x_2 (\\hat{y}_2 - y_2) & x_2 (\\hat{y}_3 - y_3) & x_2 (\\hat{y}_4 - y_4) \\\\ x_3 (\\hat{y}_1 - y_1) & x_3 (\\hat{y}_2 - y_2) & x_3 (\\hat{y}_3 - y_3) & x_3 (\\hat{y}_4 - y_4) \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Plugging in the values:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{x} = [2, 1, 3], \\quad \\hat{\\mathbf{Y}} = [0.0819, 0.1492, 0.2724, 0.4965], \\quad \\mathbf{Y} = [0, 0, 0, 1]\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\hat{\\mathbf{Y}} - \\mathbf{Y} = [0.0819, 0.1492, 0.2724, -0.5035]\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{CCE}}{\\partial \\mathbf{W}} = \\begin{bmatrix}\n",
    "   2 \\times 0.0819 & 2 \\times 0.1492 & 2 \\times 0.2724 & 2 \\times (-0.5035) \\\\\n",
    "   1 \\times 0.0819 & 1 \\times 0.1492 & 1 \\times 0.2724 & 1 \\times (-0.5035) \\\\\n",
    "   3 \\times 0.0819 & 3 \\times 0.1492 & 3 \\times 0.2724 & 3 \\times (-0.5035) \\\\\n",
    "   \\end{bmatrix} = \\begin{bmatrix}\n",
    "   0.1638 & 0.2984 & 0.5448 & -1.007 \\\\\n",
    "   0.0819 & 0.1492 & 0.2724 & -0.5035 \\\\\n",
    "   0.2457 & 0.4476 & 0.8172 & -1.5105 \\\\\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "7. **Compute Gradient with Respect to Bias Vector $ \\mathbf{b} $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{CCE}}{\\partial \\mathbf{b}} = \\hat{\\mathbf{Y}} - \\mathbf{Y} = [0.0819, 0.1492, 0.2724, -0.5035]\n",
    "   $$\n",
    "\n",
    "8. **Update Weights and Bias Using Gradient Descent:**\n",
    "\n",
    "   - **Weights Update:**\n",
    "\n",
    "     $$\n",
    "     \\mathbf{W}_{\\text{new}} = \\mathbf{W} - \\eta \\cdot \\frac{\\partial \\text{CCE}}{\\partial \\mathbf{W}} = \\begin{bmatrix}\n",
    "     0.1 & 0.2 & 0.3 & 0.4 \\\\\n",
    "     0.5 & 0.6 & 0.7 & 0.8 \\\\\n",
    "     0.9 & 1.0 & 1.1 & 1.2 \\\\\n",
    "     \\end{bmatrix} - 0.1 \\cdot \\begin{bmatrix}\n",
    "     0.1638 & 0.2984 & 0.5448 & -1.007 \\\\\n",
    "     0.0819 & 0.1492 & 0.2724 & -0.5035 \\\\\n",
    "     0.2457 & 0.4476 & 0.8172 & -1.5105 \\\\\n",
    "     \\end{bmatrix} = \\begin{bmatrix}\n",
    "     0.1 - 0.01638 & 0.2 - 0.02984 & 0.3 - 0.05448 & 0.4 + 0.1007 \\\\\n",
    "     0.5 - 0.00819 & 0.6 - 0.01492 & 0.7 - 0.02724 & 0.8 + 0.05035 \\\\\n",
    "     0.9 - 0.02457 & 1.0 - 0.04476 & 1.1 - 0.08172 & 1.2 + 0.15105 \\\\\n",
    "     \\end{bmatrix} \\approx \\begin{bmatrix}\n",
    "     0.08362 & 0.17016 & 0.24552 & 0.5007 \\\\\n",
    "     0.49181 & 0.58508 & 0.67276 & 0.85035 \\\\\n",
    "     0.87543 & 0.95524 & 1.01828 & 1.35105 \\\\\n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "   - **Bias Update:**\n",
    "\n",
    "     $$\n",
    "     \\mathbf{b}_{\\text{new}} = \\mathbf{b} - \\eta \\cdot \\frac{\\partial \\text{CCE}}{\\partial \\mathbf{b}} = [0, 0, 0, 0] - 0.1 \\cdot [0.0819, 0.1492, 0.2724, -0.5035] = [-0.00819, -0.01492, -0.02724, 0.05035]\n",
    "     $$\n",
    "\n",
    "**Interpretation:**\n",
    "- The weights and biases have been adjusted to increase the predicted probability $ \\hat{y}_3 $ for the true class (Digit 3) and decrease the probabilities for other classes, thereby reducing the loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conclusion\n",
    "\n",
    "Understanding **Binary Cross-Entropy (BCE)** and **Categorical Cross-Entropy (CCE)** loss functions is fundamental for effectively training classification models in machine learning. Here's a summary of key points:\n",
    "\n",
    "- **Binary Cross-Entropy (BCE):**\n",
    "  - **Use Case:** Binary classification tasks.\n",
    "  - **Activation Function:** Sigmoid, providing a single probability output.\n",
    "  - **Loss Function:** Measures the discrepancy between true binary labels and predicted probabilities.\n",
    "  - **Gradient Derivation:** Results in $ \\hat{y} - y $, guiding weight updates to minimize loss.\n",
    "\n",
    "- **Categorical Cross-Entropy (CCE):**\n",
    "  - **Use Case:** Multi-class classification tasks with mutually exclusive classes.\n",
    "  - **Activation Function:** Softmax, providing a probability distribution over classes.\n",
    "  - **Loss Function:** Measures the discrepancy between true one-hot labels and predicted probability distributions.\n",
    "  - **Gradient Derivation:** Results in $ \\hat{y}_k - y_k $ for each class $ k $, guiding weight updates to minimize loss.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Activation and Loss Function Pairing:**\n",
    "   - **BCE with Sigmoid:** Ideal for scenarios with two classes, ensuring output probabilities align with loss expectations.\n",
    "   - **CCE with Softmax:** Perfect for scenarios with multiple classes, ensuring output probabilities form a valid distribution over classes.\n",
    "\n",
    "2. **Gradient Computations:**\n",
    "   - Accurate gradient derivations are crucial for effective optimization.\n",
    "   - Both BCE and CCE gradients involve the difference between predicted probabilities and true labels, enabling models to adjust parameters to better fit the data.\n",
    "\n",
    "3. **Model Training:**\n",
    "   - Utilizing these loss functions with appropriate activation functions facilitates efficient and effective learning, driving models to make accurate predictions.\n",
    "\n",
    "By meticulously understanding the mathematical foundations and practical implementations of BCE and CCE, you can design and train robust classification models tailored to a wide array of machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Features (X):\n",
      " [[[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]\n",
      "  [-0.23413696  1.57921282  0.76743473 -0.46947439  0.54256004]\n",
      "  [-0.46341769 -0.46572975  0.24196227 -1.91328024 -1.72491783]]\n",
      "\n",
      " [[-0.56228753 -1.01283112  0.31424733 -0.90802408 -1.4123037 ]\n",
      "  [ 1.46564877 -0.2257763   0.0675282  -1.42474819 -0.54438272]\n",
      "  [ 0.11092259 -1.15099358  0.37569802 -0.60063869 -0.29169375]]]\n",
      "\n",
      "Targets:\n",
      " [[1 0 1]\n",
      " [0 1 0]]\n",
      "\n",
      "Initial Weights (W):\n",
      " [[ 0.18423303]\n",
      " [-0.05984751]\n",
      " [-0.37796177]\n",
      " [-0.00482309]\n",
      " [-0.46561148]]\n",
      "\n",
      "Initial Biases (b):\n",
      " [0.4093204]\n",
      "Epoch    1: Loss = 0.6569, Accuracy = 66.67%\n",
      "Epoch  100: Loss = 0.4270, Accuracy = 100.00%\n",
      "Epoch  200: Loss = 0.3402, Accuracy = 100.00%\n",
      "Epoch  300: Loss = 0.2867, Accuracy = 100.00%\n",
      "Epoch  400: Loss = 0.2499, Accuracy = 100.00%\n",
      "Epoch  500: Loss = 0.2228, Accuracy = 100.00%\n",
      "Epoch  600: Loss = 0.2018, Accuracy = 100.00%\n",
      "Epoch  700: Loss = 0.1850, Accuracy = 100.00%\n",
      "Epoch  800: Loss = 0.1711, Accuracy = 100.00%\n",
      "Epoch  900: Loss = 0.1593, Accuracy = 100.00%\n",
      "Epoch 1000: Loss = 0.1493, Accuracy = 100.00%\n",
      "\n",
      "Training Complete!\n",
      "Final Loss: 0.1493\n",
      "Final Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Data Preparation\n",
    "# -------------------------------\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample input data: shape [batch_size, seq_length, input_dim]\n",
    "# Randomly generated float values\n",
    "X = np.random.randn(2, 3, 5)  # 2 sequences, 3 tokens each, 5 features per token\n",
    "\n",
    "# Sample targets: shape [batch_size, seq_length]\n",
    "# Each target is a binary value (0 or 1) representing the true class\n",
    "targets = np.array([\n",
    "    [1, 0, 1],  # First sequence's true classes\n",
    "    [0, 1, 0]   # Second sequence's true classes\n",
    "])\n",
    "\n",
    "print(\"Input Features (X):\\n\", X)\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Parameter Initialization\n",
    "# -------------------------------\n",
    "\n",
    "# Parameters\n",
    "batch_size, seq_length, input_dim = X.shape  # (2, 3, 5)\n",
    "output_dim = 1  # Binary classification\n",
    "\n",
    "# Initialize weights (W) and biases (b)\n",
    "W = np.random.uniform(-0.5, 0.5, (input_dim, output_dim))  # Shape: [5, 1]\n",
    "b = np.random.uniform(-0.5, 0.5, output_dim)              # Shape: [1]\n",
    "\n",
    "print(\"\\nInitial Weights (W):\\n\", W)\n",
    "print(\"\\nInitial Biases (b):\\n\", b)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Activation and Loss Functions\n",
    "# -------------------------------\n",
    "\n",
    "def sigmoid(logits):\n",
    "    \"\"\"\n",
    "    Applies the sigmoid function to the logits.\n",
    "    \n",
    "    Args:\n",
    "        logits (np.ndarray): Logits array of any shape.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Sigmoid probabilities of the same shape as logits.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-logits))\n",
    "\n",
    "def binary_cross_entropy_loss(probs, targets):\n",
    "    \"\"\"\n",
    "    Computes the average binary cross-entropy loss over the batch.\n",
    "    \n",
    "    Args:\n",
    "        probs (np.ndarray): Predicted probabilities of shape [batch_size, seq_length, 1]\n",
    "        targets (np.ndarray): True binary labels of shape [batch_size, seq_length]\n",
    "        \n",
    "    Returns:\n",
    "        float: Average binary cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    n = batch_size * seq_length\n",
    "    \n",
    "    # Clip probabilities to prevent log(0)\n",
    "    epsilon = 1e-12\n",
    "    probs = np.clip(probs, epsilon, 1. - epsilon)\n",
    "    \n",
    "    # Reshape targets to match probs shape\n",
    "    targets = targets.reshape(batch_size, seq_length, 1)\n",
    "    \n",
    "    # Compute binary cross-entropy loss\n",
    "    loss = -np.sum(targets * np.log(probs) + (1 - targets) * np.log(1 - probs)) / n\n",
    "    return loss\n",
    "\n",
    "def compute_accuracy(probs, targets):\n",
    "    \"\"\"\n",
    "    Computes the accuracy over the batch.\n",
    "    \n",
    "    Args:\n",
    "        probs (np.ndarray): Predicted probabilities of shape [batch_size, seq_length, 1]\n",
    "        targets (np.ndarray): True binary labels of shape [batch_size, seq_length]\n",
    "        \n",
    "    Returns:\n",
    "        float: Accuracy (between 0 and 1)\n",
    "    \"\"\"\n",
    "    # Convert probabilities to binary predictions\n",
    "    predictions = (probs >= 0.5).astype(int).reshape(batch_size, seq_length)\n",
    "    correct = (predictions == targets).astype(float)\n",
    "    accuracy = np.mean(correct)\n",
    "    return accuracy\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Training Loop\n",
    "# -------------------------------\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # ---------------------------\n",
    "    # Forward Pass\n",
    "    # ---------------------------\n",
    "    \n",
    "    # Compute logits: [batch_size, seq_length, output_dim]\n",
    "    # X: [batch_size, seq_length, input_dim]\n",
    "    # W: [input_dim, output_dim]\n",
    "    # b: [output_dim]\n",
    "    logits = np.dot(X, W) + b  # Broadcasting b to [batch_size, seq_length, output_dim]\n",
    "    \n",
    "    # Compute probabilities using sigmoid\n",
    "    probs = sigmoid(logits)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = binary_cross_entropy_loss(probs, targets)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = compute_accuracy(probs, targets)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Backward Pass (Gradient Computation)\n",
    "    # ---------------------------\n",
    "    \n",
    "    # Number of samples\n",
    "    n = batch_size * seq_length\n",
    "    \n",
    "    # Reshape targets to match probs shape\n",
    "    targets_reshaped = targets.reshape(batch_size, seq_length, 1)\n",
    "    \n",
    "    # Gradient of loss w.r.t logits\n",
    "    dL_dlogits = (probs - targets_reshaped) / n  # Shape: [batch_size, seq_length, 1]\n",
    "    \n",
    "    # Gradient w.r.t W: [input_dim, output_dim]\n",
    "    dL_dW = np.dot(X.reshape(-1, input_dim).T, dL_dlogits.reshape(-1, output_dim))\n",
    "    \n",
    "    # Gradient w.r.t b: [output_dim]\n",
    "    dL_db = np.sum(dL_dlogits, axis=(0,1))\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Parameter Update\n",
    "    # ---------------------------\n",
    "    \n",
    "    W -= learning_rate * dL_dW\n",
    "    b -= learning_rate * dL_db\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Logging\n",
    "    # ---------------------------\n",
    "    \n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:4d}: Loss = {loss:.4f}, Accuracy = {accuracy*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Final Evaluation\n",
    "# -------------------------------\n",
    "\n",
    "print(\"\\nTraining Complete!\")\n",
    "print(f\"Final Loss: {loss:.4f}\")\n",
    "print(f\"Final Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCE\n",
    "\n",
    "Refer to [ML_0047_Gradient_Descent_Variants.ipynb](./data_structures/ML_0047_Gradient_Descent_Variants.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Features (X):\n",
      " [[[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]\n",
      "  [-0.23413696  1.57921282  0.76743473 -0.46947439  0.54256004]\n",
      "  [-0.46341769 -0.46572975  0.24196227 -1.91328024 -1.72491783]]\n",
      "\n",
      " [[-0.56228753 -1.01283112  0.31424733 -0.90802408 -1.4123037 ]\n",
      "  [ 1.46564877 -0.2257763   0.0675282  -1.42474819 -0.54438272]\n",
      "  [ 0.11092259 -1.15099358  0.37569802 -0.60063869 -0.29169375]]]\n",
      "\n",
      "Targets:\n",
      " [[0 1 3]\n",
      " [2 4 1]]\n",
      "\n",
      "Initial Weights (W):\n",
      " [[ 0.18423303 -0.05984751 -0.37796177 -0.00482309 -0.46561148]\n",
      " [ 0.4093204  -0.24122002  0.16252228 -0.18828892  0.02006802]\n",
      " [ 0.04671028 -0.31514554  0.46958463  0.27513282  0.43949894]\n",
      " [ 0.39482735  0.09789998  0.42187424 -0.4115075  -0.30401714]\n",
      " [-0.45477271 -0.17466967 -0.11132271 -0.22865097  0.32873751]]\n",
      "\n",
      "Initial Biases (b):\n",
      " [-0.14324667 -0.21906549  0.04269608 -0.35907578  0.30219698]\n",
      "Epoch    1: Loss = 1.6869, Accuracy = 16.67%\n",
      "Epoch  100: Loss = 0.6148, Accuracy = 83.33%\n",
      "Epoch  200: Loss = 0.3972, Accuracy = 100.00%\n",
      "Epoch  300: Loss = 0.2862, Accuracy = 100.00%\n",
      "Epoch  400: Loss = 0.2208, Accuracy = 100.00%\n",
      "Epoch  500: Loss = 0.1786, Accuracy = 100.00%\n",
      "Epoch  600: Loss = 0.1496, Accuracy = 100.00%\n",
      "Epoch  700: Loss = 0.1284, Accuracy = 100.00%\n",
      "Epoch  800: Loss = 0.1124, Accuracy = 100.00%\n",
      "Epoch  900: Loss = 0.0999, Accuracy = 100.00%\n",
      "Epoch 1000: Loss = 0.0899, Accuracy = 100.00%\n",
      "\n",
      "Training Complete!\n",
      "Final Loss: 0.0899\n",
      "Final Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Data Preparation\n",
    "# -------------------------------\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample input data: shape [batch_size, seq_length, input_dim]\n",
    "# Randomly generated float values\n",
    "X = np.random.randn(2, 3, 5)  # 2 sequences, 3 tokens each, 5 features per token\n",
    "\n",
    "# Sample targets: shape [batch_size, seq_length]\n",
    "# Each target is an integer representing the true class index\n",
    "targets = np.array([\n",
    "    [0, 1, 3],  # First sequence's true classes\n",
    "    [2, 4, 1]   # Second sequence's true classes\n",
    "])\n",
    "\n",
    "print(\"Input Features (X):\\n\", X)\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Parameter Initialization\n",
    "# -------------------------------\n",
    "\n",
    "# Parameters\n",
    "batch_size, seq_length, input_dim = X.shape  # (2, 3, 5)\n",
    "vocab_size = 5  # Number of classes\n",
    "\n",
    "# Initialize weights (W) and biases (b)\n",
    "W = np.random.uniform(-0.5, 0.5, (input_dim, vocab_size))  # Shape: [5, 5]\n",
    "b = np.random.uniform(-0.5, 0.5, vocab_size)              # Shape: [5]\n",
    "\n",
    "print(\"\\nInitial Weights (W):\\n\", W)\n",
    "print(\"\\nInitial Biases (b):\\n\", b)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Activation and Loss Functions\n",
    "# -------------------------------\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Applies the softmax function to the logits.\n",
    "    \n",
    "    Args:\n",
    "        logits (np.ndarray): Logits array of shape (..., vocab_size)\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Softmax probabilities of the same shape as logits\n",
    "    \"\"\"\n",
    "    # For numerical stability, subtract the max logit from each logit\n",
    "    shifted_logits = logits - np.max(logits, axis=-1, keepdims=True)\n",
    "    exp_logits = np.exp(shifted_logits)\n",
    "    sum_exp = np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    return exp_logits / sum_exp\n",
    "\n",
    "def cross_entropy_loss(probs, targets):\n",
    "    \"\"\"\n",
    "    Computes the average cross-entropy loss over the batch.\n",
    "    \n",
    "    Args:\n",
    "        probs (np.ndarray): Predicted probabilities of shape [batch_size, seq_length, vocab_size]\n",
    "        targets (np.ndarray): True class indices of shape [batch_size, seq_length]\n",
    "        \n",
    "    Returns:\n",
    "        float: Average cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    n = batch_size * seq_length\n",
    "    \n",
    "    # Clip probabilities to prevent log(0)\n",
    "    epsilon = 1e-12\n",
    "    probs = np.clip(probs, epsilon, 1. - epsilon)\n",
    "    \n",
    "    # Create one-hot encoding for targets\n",
    "    one_hot_targets = np.zeros_like(probs)\n",
    "    \n",
    "    # Generate batch_indices and seq_indices\n",
    "    batch_indices = np.repeat(np.arange(batch_size), seq_length)\n",
    "    seq_indices = np.tile(np.arange(seq_length), batch_size)\n",
    "    \n",
    "    # Flatten targets for indexing\n",
    "    flat_targets = targets.flatten()\n",
    "    \n",
    "    # Assign 1s to the correct class positions\n",
    "    for b in range(batch_size):\n",
    "        for s in range(seq_length):\n",
    "            one_hot_targets[b, s, targets[b, s]] = 1\n",
    "    # Compute cross-entropy loss\n",
    "    loss = -np.sum(one_hot_targets * np.log(probs)) / n\n",
    "    return loss\n",
    "\n",
    "def compute_accuracy(probs, targets):\n",
    "    \"\"\"\n",
    "    Computes the accuracy over the batch.\n",
    "    \n",
    "    Args:\n",
    "        probs (np.ndarray): Predicted probabilities of shape [batch_size, seq_length, vocab_size]\n",
    "        targets (np.ndarray): True class indices of shape [batch_size, seq_length]\n",
    "        \n",
    "    Returns:\n",
    "        float: Accuracy (between 0 and 1)\n",
    "    \"\"\"\n",
    "    predictions = np.argmax(probs, axis=-1)\n",
    "    correct = (predictions == targets).astype(float)\n",
    "    accuracy = np.mean(correct)\n",
    "    return accuracy\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Training Loop\n",
    "# -------------------------------\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # ---------------------------\n",
    "    # Forward Pass\n",
    "    # ---------------------------\n",
    "    \n",
    "    # Compute logits: [batch_size, seq_length, vocab_size]\n",
    "    # X: [batch_size, seq_length, input_dim]\n",
    "    # W: [input_dim, vocab_size]\n",
    "    # b: [vocab_size]\n",
    "    logits = np.dot(X, W) + b  # Broadcasting b to [batch_size, seq_length, vocab_size]\n",
    "    \n",
    "    # Compute probabilities using softmax\n",
    "    probs = softmax(logits)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = cross_entropy_loss(probs, targets)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = compute_accuracy(probs, targets)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Backward Pass (Gradient Computation)\n",
    "    # ---------------------------\n",
    "    \n",
    "    # Number of samples\n",
    "    n = batch_size * seq_length\n",
    "    \n",
    "    # Create one-hot encoding for targets\n",
    "    one_hot_targets = np.zeros_like(probs)\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for s in range(seq_length):\n",
    "            one_hot_targets[b, s, targets[b, s]] = 1\n",
    "            \n",
    "    # Gradient of loss w.r.t logits\n",
    "    dL_dlogits = (probs - one_hot_targets) / n  # Shape: [batch_size, seq_length, vocab_size]\n",
    "    \n",
    "    # Gradient w.r.t W: [input_dim, vocab_size]\n",
    "    dL_dW = np.dot(X.reshape(-1, input_dim).T, dL_dlogits.reshape(-1, vocab_size))\n",
    "    \n",
    "    # Gradient w.r.t b: [vocab_size]\n",
    "    dL_db = np.sum(dL_dlogits, axis=(0,1))\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Parameter Update\n",
    "    # ---------------------------\n",
    "    \n",
    "    W -= learning_rate * dL_dW\n",
    "    b -= learning_rate * dL_db\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Logging\n",
    "    # ---------------------------\n",
    "    \n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:4d}: Loss = {loss:.4f}, Accuracy = {accuracy*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Final Evaluation\n",
    "# -------------------------------\n",
    "\n",
    "print(\"\\nTraining Complete!\")\n",
    "print(f\"Final Loss: {loss:.4f}\")\n",
    "print(f\"Final Accuracy: {accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leetcode_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
