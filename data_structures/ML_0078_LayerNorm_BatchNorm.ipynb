{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization: Step-by-Step Explanation and Implementation\n",
    "\n",
    "Layer Normalization is a technique used in deep learning to normalize the inputs of a layer across its features, ensuring that the input distribution has a mean of 0 and variance of 1 for each sample.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "Given an input $ x $ of shape $ (\\text{batch size}, \\text{num features}) $, Layer Normalization works as follows:\n",
    "\n",
    "### Step 1: Calculate the Mean\n",
    "For each sample (row), compute the mean across all features:\n",
    "\n",
    "$$\n",
    "\\text{mean} = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ n $: Number of features.\n",
    "- $ x_i $: Value of the $ i $-th feature.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Calculate the Variance\n",
    "For each sample (row), compute the variance across all features:\n",
    "\n",
    "$$\n",
    "\\text{variance} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\text{mean})^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Normalize\n",
    "Normalize each feature by subtracting the mean and dividing by the square root of the variance (plus a small $ \\epsilon $ to avoid division by zero):\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\text{mean}}{\\sqrt{\\text{variance} + \\epsilon}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\hat{x}_i $: The normalized value of $ x_i $.\n",
    "- $ \\epsilon $: A small constant (e.g., $ 1 \\times 10^{-5} $) to prevent division by zero.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix:\n",
      "[1.0, 2.0, 3.0]\n",
      "[4.0, 5.0, 6.0]\n",
      "[7.0, 8.0, 9.0]\n",
      "\n",
      "Normalized Matrix:\n",
      "[-1.2247356859083902, 0.0, 1.2247356859083902]\n",
      "[-1.2247356859083902, 0.0, 1.2247356859083902]\n",
      "[-1.2247356859083902, 0.0, 1.2247356859083902]\n"
     ]
    }
   ],
   "source": [
    "def layernorm(x, epsilon=1e-5):\n",
    "    # Initialize the output\n",
    "    output = []\n",
    "\n",
    "    # Iterate over each sample in the batch\n",
    "    for sample in x:\n",
    "        # Calculate the mean of the sample\n",
    "        mean = sum(sample) / len(sample)\n",
    "\n",
    "        # Calculate the variance of the sample\n",
    "        variance = sum((xi - mean) ** 2 for xi in sample) / len(sample)\n",
    "\n",
    "        # Normalize each element in the sample\n",
    "        normalized_sample = [(xi - mean) / ((variance + epsilon) ** 0.5) for xi in sample]\n",
    "\n",
    "        # Append the normalized sample to the output\n",
    "        output.append(normalized_sample)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "x = [\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0]\n",
    "]\n",
    "\n",
    "normalized_x = layernorm(x)\n",
    "\n",
    "print(\"Input Matrix:\")\n",
    "for row in x:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nNormalized Matrix:\")\n",
    "for row in normalized_x:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization: Step-by-Step Explanation and Implementation\n",
    "\n",
    "Batch Normalization (BatchNorm) is a technique used to normalize the input of a neural network layer across a batch of data. It helps stabilize training, speeds up convergence, and reduces sensitivity to initialization.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "Given an input matrix $ x $ of shape $ (\\text{batch size}, \\text{num features}) $, Batch Normalization works as follows:\n",
    "\n",
    "### Step 1: Calculate the Mean for Each Feature\n",
    "For each feature $ f $, compute the mean across the batch:\n",
    "\n",
    "$$\n",
    "\\text{mean}_f = \\frac{1}{N} \\sum_{i=1}^{N} x_{i,f}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ N $: Number of samples in the batch (batch size).\n",
    "- $ x_{i,f} $: The value of the $ f $-th feature for the $ i $-th sample.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Calculate the Variance for Each Feature\n",
    "For each feature $ f $, compute the variance across the batch:\n",
    "\n",
    "$$\n",
    "\\text{variance}_f = \\frac{1}{N} \\sum_{i=1}^{N} (x_{i,f} - \\text{mean}_f)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Normalize Each Feature\n",
    "Normalize each feature using the computed mean and variance:\n",
    "\n",
    "$$\n",
    "\\hat{x}_{i,f} = \\frac{x_{i,f} - \\text{mean}_f}{\\sqrt{\\text{variance}_f + \\epsilon}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\epsilon $: A small constant (e.g., $ 10^{-5} $) added to avoid division by zero.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Apply Learnable Parameters $ \\gamma $ and $ \\beta $\n",
    "Finally, scale and shift the normalized features using learnable parameters $ \\gamma $ and $ \\beta $:\n",
    "\n",
    "$$\n",
    "y_{i,f} = \\gamma_f \\cdot \\hat{x}_{i,f} + \\beta_f\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\gamma_f $: Learnable scaling parameter for feature $ f $.\n",
    "- $ \\beta_f $: Learnable shifting parameter for feature $ f $.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix:\n",
      "[1.0, 2.0, 3.0]\n",
      "[4.0, 5.0, 6.0]\n",
      "[7.0, 8.0, 9.0]\n",
      "\n",
      "Batch-Normalized Matrix:\n",
      "[-1.2247438507721387, -1.2247438507721387, -1.2247438507721387]\n",
      "[0.0, 0.0, 0.0]\n",
      "[1.2247438507721387, 1.2247438507721387, 1.2247438507721387]\n"
     ]
    }
   ],
   "source": [
    "def batchnorm(x, gamma, beta, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Implements Batch Normalization from scratch.\n",
    "\n",
    "    Args:\n",
    "        x (list of lists or 2D array): Input matrix (batch_size x num_features).\n",
    "        gamma (list): Learnable scaling parameters (1D array of size num_features).\n",
    "        beta (list): Learnable shifting parameters (1D array of size num_features).\n",
    "        epsilon (float): Small constant to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        list of lists: Batch-normalized matrix (same shape as input).\n",
    "    \"\"\"\n",
    "    # Transpose the input to operate on features across the batch\n",
    "    x_transposed = list(zip(*x))  # Shape: (num_features, batch_size)\n",
    "\n",
    "    # Initialize the output\n",
    "    normalized_x_transposed = []\n",
    "\n",
    "    # Iterate over each feature\n",
    "    for feature_idx, feature_values in enumerate(x_transposed):\n",
    "        # Step 1: Calculate the mean for the feature\n",
    "        mean = sum(feature_values) / len(feature_values)\n",
    "\n",
    "        # Step 2: Calculate the variance for the feature\n",
    "        variance = sum((xi - mean) ** 2 for xi in feature_values) / len(feature_values)\n",
    "\n",
    "        # Step 3: Normalize the feature\n",
    "        normalized_feature = [(xi - mean) / ((variance + epsilon) ** 0.5) for xi in feature_values]\n",
    "\n",
    "        # Step 4: Apply gamma and beta (scaling and shifting)\n",
    "        scaled_and_shifted = [gamma[feature_idx] * normalized_value + beta[feature_idx]\n",
    "                              for normalized_value in normalized_feature]\n",
    "\n",
    "        # Append the processed feature\n",
    "        normalized_x_transposed.append(scaled_and_shifted)\n",
    "\n",
    "    # Transpose back to the original shape\n",
    "    normalized_x = list(zip(*normalized_x_transposed))\n",
    "\n",
    "    # Convert tuples back to lists\n",
    "    return [list(row) for row in normalized_x]\n",
    "\n",
    "# Example usage\n",
    "x = [\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0]\n",
    "]\n",
    "\n",
    "# Learnable parameters (gamma and beta for scaling and shifting)\n",
    "gamma = [1.0, 1.0, 1.0]  # Scaling factors\n",
    "beta = [0.0, 0.0, 0.0]   # Shifting factors\n",
    "\n",
    "# Apply Batch Normalization\n",
    "normalized_x = batchnorm(x, gamma, beta)\n",
    "\n",
    "print(\"Input Matrix:\")\n",
    "for row in x:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nBatch-Normalized Matrix:\")\n",
    "for row in normalized_x:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Between BatchNorm and LayerNorm\n",
    "\n",
    "Batch Normalization (BatchNorm) and Layer Normalization (LayerNorm) are two widely used normalization techniques in deep learning. While they serve similar purposes, they differ significantly in how they normalize data and in their use cases.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences Between BatchNorm and LayerNorm\n",
    "\n",
    "| **Aspect**           | **Batch Normalization (BatchNorm)**                                                                     | **Layer Normalization (LayerNorm)**                                                                      |\n",
    "|-----------------------|--------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\n",
    "| **Normalization Axis**| Normalizes across the batch dimension (for each feature across all samples in the batch).             | Normalizes across the feature dimension (for each sample across all features).                          |\n",
    "| **Formula**           | $\\mu_f = \\frac{1}{N} \\sum_{i=1}^N x_{i,f}, \\sigma_f^2 = \\frac{1}{N} \\sum_{i=1}^N (x_{i,f} - \\mu_f)^2$ | $\\mu_s = \\frac{1}{n} \\sum_{j=1}^n x_{s,j}, \\sigma_s^2 = \\frac{1}{n} \\sum_{j=1}^n (x_{s,j} - \\mu_s)^2$ |\n",
    "| **Use Case**          | Commonly used in Convolutional Neural Networks (CNNs) and feedforward networks during mini-batch training.| Suitable for sequence models (e.g., RNNs, Transformers) or networks with variable batch sizes.          |\n",
    "| **Dependency on Batch Size**| Requires a consistent batch size to work effectively.                                            | Works independently of the batch size, making it suitable for single-sample inference or varying batch sizes. |\n",
    "| **Learnable Parameters** | Scaling ($\\gamma$) and shifting ($\\beta$) parameters per feature.                              | Scaling ($\\gamma$) and shifting ($\\beta$) parameters per feature in each sample.                     |\n",
    "| **When Normalization Occurs** | Normalizes features across samples in the same batch.                                         | Normalizes features within each individual sample.                                                       |\n",
    "| **Computation**       | Operates on $(\\text{batch size} \\times \\text{num features})$ and normalizes per feature across the batch.| Operates on $(\\text{batch size} \\times \\text{num features})$ and normalizes across features for each sample. |\n",
    "| **Advantages**        | Stabilizes training and reduces sensitivity to initialization, especially in convolutional layers.     | Useful in NLP and sequence models where batch sizes vary, and consistent behavior is required.           |\n",
    "| **Disadvantages**     | May not work well for very small batch sizes or single-sample inference.                               | Computationally more expensive compared to BatchNorm for large inputs.                                   |\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Formulas\n",
    "\n",
    "### Batch Normalization\n",
    "For each feature $ f $, BatchNorm operates as:\n",
    "\n",
    "1. Compute mean:\n",
    "   $$\n",
    "   \\mu_f = \\frac{1}{N} \\sum_{i=1}^N x_{i,f}\n",
    "   $$\n",
    "2. Compute variance:\n",
    "   $$\n",
    "   \\sigma_f^2 = \\frac{1}{N} \\sum_{i=1}^N (x_{i,f} - \\mu_f)^2\n",
    "   $$\n",
    "3. Normalize:\n",
    "   $$\n",
    "   \\hat{x}_{i,f} = \\frac{x_{i,f} - \\mu_f}{\\sqrt{\\sigma_f^2 + \\epsilon}}\n",
    "   $$\n",
    "4. Scale and shift:\n",
    "   $$\n",
    "   y_{i,f} = \\gamma_f \\cdot \\hat{x}_{i,f} + \\beta_f\n",
    "   $$\n",
    "\n",
    "Where:\n",
    "- $ N $: Batch size.\n",
    "- $ f $: Feature index.\n",
    "\n",
    "---\n",
    "\n",
    "### Layer Normalization\n",
    "For each sample $ s $, LayerNorm operates as:\n",
    "\n",
    "1. Compute mean:\n",
    "   $$\n",
    "   \\mu_s = \\frac{1}{n} \\sum_{j=1}^n x_{s,j}\n",
    "   $$\n",
    "2. Compute variance:\n",
    "   $$\n",
    "   \\sigma_s^2 = \\frac{1}{n} \\sum_{j=1}^n (x_{s,j} - \\mu_s)^2\n",
    "   $$\n",
    "3. Normalize:\n",
    "   $$\n",
    "   \\hat{x}_{s,j} = \\frac{x_{s,j} - \\mu_s}{\\sqrt{\\sigma_s^2 + \\epsilon}}\n",
    "   $$\n",
    "4. Scale and shift:\n",
    "   $$\n",
    "   y_{s,j} = \\gamma_j \\cdot \\hat{x}_{s,j} + \\beta_j\n",
    "   $$\n",
    "\n",
    "Where:\n",
    "- $ n $: Number of features.\n",
    "- $ s $: Sample index.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Differences\n",
    "\n",
    "### BatchNorm Example:\n",
    "Given an input matrix:\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix}\n",
    "1.0 & 2.0 & 3.0 \\\\\n",
    "4.0 & 5.0 & 6.0 \\\\\n",
    "7.0 & 8.0 & 9.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "BatchNorm normalizes across the **batch** (i.e., for each column or feature).\n",
    "\n",
    "### LayerNorm Example:\n",
    "Using the same input matrix:\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix}\n",
    "1.0 & 2.0 & 3.0 \\\\\n",
    "4.0 & 5.0 & 6.0 \\\\\n",
    "7.0 & 8.0 & 9.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "LayerNorm normalizes across the **features** (i.e., for each row or sample).\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### BatchNorm:\n",
    "- Normalizes **across the batch** for each feature.\n",
    "- Works best for **CNNs and feedforward networks**.\n",
    "- Relies on a consistent batch size.\n",
    "\n",
    "### LayerNorm:\n",
    "- Normalizes **within each sample** across features.\n",
    "- Preferred for **sequence models** (e.g., RNNs, Transformers).\n",
    "- Independent of batch size, making it robust for single-sample inputs.\n",
    "\n",
    "By understanding these differences, you can choose the appropriate normalization technique based on your model's requirements.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leetcode_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
