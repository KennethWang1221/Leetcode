{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple demo for LLM training\n",
    "\n",
    "A simplified Transformer-based Language Model using NumPy. \n",
    "\n",
    "Note: This implementation uses mocked gradients and does not perform actual backpropagation for all components. \n",
    "\n",
    "Implementing true gradients for a Transformer manually is highly complex and not practical; hence, this script serves educational purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequences (X):\n",
      " [[1 2 3]\n",
      " [2 4 1]]\n",
      "\n",
      "Target Sequences:\n",
      " [[2 3 4]\n",
      " [4 1 2]]\n",
      "Epoch    1: Loss = 1.8555, Accuracy = 33.33%\n",
      "Epoch  100: Loss = 1.1927, Accuracy = 50.00%\n",
      "Epoch  200: Loss = 0.8908, Accuracy = 83.33%\n",
      "Epoch  300: Loss = 0.7225, Accuracy = 100.00%\n",
      "Epoch  400: Loss = 0.6155, Accuracy = 100.00%\n",
      "Epoch  500: Loss = 0.5397, Accuracy = 100.00%\n",
      "Epoch  600: Loss = 0.4822, Accuracy = 100.00%\n",
      "Epoch  700: Loss = 0.4366, Accuracy = 100.00%\n",
      "Epoch  800: Loss = 0.3991, Accuracy = 100.00%\n",
      "Epoch  900: Loss = 0.3677, Accuracy = 100.00%\n",
      "Epoch 1000: Loss = 0.3408, Accuracy = 100.00%\n",
      "\n",
      "Sequence 1:\n",
      "Input:    A B C\n",
      "Target:   B C D\n",
      "Predicted:B C D\n",
      "\n",
      "Sequence 2:\n",
      "Input:    B D A\n",
      "Target:   D A B\n",
      "Predicted:D A B\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Dataset Preparation\n",
    "# -------------------------------\n",
    "\n",
    "# Define vocabulary\n",
    "vocab = ['<PAD>', 'A', 'B', 'C', 'D']\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Sample input sequences: [batch_size, seq_length]\n",
    "X = np.array([\n",
    "    [1, 2, 3],  # First sequence: A B C\n",
    "    [2, 4, 1]   # Second sequence: B D A\n",
    "])\n",
    "\n",
    "# Sample target sequences: [batch_size, seq_length]\n",
    "targets = np.array([\n",
    "    [2, 3, 4],  # Targets: B C D\n",
    "    [4, 1, 2]   # Targets: D A B\n",
    "])\n",
    "\n",
    "print(\"Input Sequences (X):\\n\", X)\n",
    "print(\"\\nTarget Sequences:\\n\", targets)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Model Components\n",
    "# -------------------------------\n",
    "\n",
    "# Positional Encoding\n",
    "def get_positional_encoding(seq_length, embed_size):\n",
    "    PE = np.zeros((seq_length, embed_size))\n",
    "    for pos in range(seq_length):\n",
    "        for i in range(0, embed_size, 2):\n",
    "            PE[pos, i] = np.sin(pos / (10000 ** ((2 * i)/embed_size)))\n",
    "            if i + 1 < embed_size:\n",
    "                PE[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1))/embed_size)))\n",
    "    return PE\n",
    "\n",
    "class MultiHeadSelfAttention:\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        assert embed_size % num_heads == 0, \"Embedding size must be divisible by number of heads.\"\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        \n",
    "        # Initialize weight matrices for Q, K, V\n",
    "        self.W_Q = np.random.randn(embed_size, embed_size) / np.sqrt(embed_size)\n",
    "        self.W_K = np.random.randn(embed_size, embed_size) / np.sqrt(embed_size)\n",
    "        self.W_V = np.random.randn(embed_size, embed_size) / np.sqrt(embed_size)\n",
    "        \n",
    "        # Output weight matrix\n",
    "        self.W_O = np.random.randn(embed_size, embed_size) / np.sqrt(embed_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head self-attention.\n",
    "        \n",
    "        Args:\n",
    "            X: Input embeddings of shape [batch_size, seq_length, embed_size]\n",
    "        \n",
    "        Returns:\n",
    "            Output after attention of shape [batch_size, seq_length, embed_size]\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, embed_size = X.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = X @ self.W_Q  # [batch_size, seq_length, embed_size]\n",
    "        K = X @ self.W_K\n",
    "        V = X @ self.W_V\n",
    "        \n",
    "        # Split into heads\n",
    "        Q = Q.reshape(batch_size, seq_length, self.num_heads, self.head_dim).transpose(0,2,1,3)  # [batch, heads, seq, head_dim]\n",
    "        K = K.reshape(batch_size, seq_length, self.num_heads, self.head_dim).transpose(0,2,1,3)\n",
    "        V = V.reshape(batch_size, seq_length, self.num_heads, self.head_dim).transpose(0,2,1,3)\n",
    "        \n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = (Q @ K.transpose(0,1,3,2)) / np.sqrt(self.head_dim)  # [batch, heads, seq, seq]\n",
    "        attn_weights = softmax(scores, axis=-1)  # [batch, heads, seq, seq]\n",
    "        attn_output = attn_weights @ V  # [batch, heads, seq, head_dim]\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(0,2,1,3).reshape(batch_size, seq_length, embed_size)  # [batch, seq, embed_size]\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = attn_output @ self.W_O  # [batch, seq, embed_size]\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FeedForward:\n",
    "    def __init__(self, embed_size, hidden_dim):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(embed_size, hidden_dim) / np.sqrt(embed_size)\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, embed_size) / np.sqrt(hidden_dim)\n",
    "        self.b2 = np.zeros(embed_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass for feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            X: Input of shape [batch_size, seq_length, embed_size]\n",
    "        \n",
    "        Returns:\n",
    "            Output of shape [batch_size, seq_length, embed_size]\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Z1 = X @ self.W1 + self.b1  # [batch, seq, hidden]\n",
    "        self.A1 = np.maximum(0, self.Z1)  # ReLU\n",
    "        self.Z2 = self.A1 @ self.W2 + self.b2  # [batch, seq, embed]\n",
    "        return self.Z2\n",
    "\n",
    "class LayerNorm:\n",
    "    def __init__(self, embed_size, epsilon=1e-6):\n",
    "        self.epsilon = epsilon\n",
    "        # Initialize scale (gamma) and shift (beta) parameters\n",
    "        self.gamma = np.ones((1, 1, embed_size))\n",
    "        self.beta = np.zeros((1, 1, embed_size))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass for layer normalization.\n",
    "        \n",
    "        Args:\n",
    "            X: Input of shape [batch_size, seq_length, embed_size]\n",
    "        \n",
    "        Returns:\n",
    "            Normalized output of same shape as X\n",
    "        \"\"\"\n",
    "        self.mean = np.mean(X, axis=-1, keepdims=True)\n",
    "        self.variance = np.var(X, axis=-1, keepdims=True)\n",
    "        self.X_norm = (X - self.mean) / np.sqrt(self.variance + self.epsilon)\n",
    "        out = self.gamma * self.X_norm + self.beta\n",
    "        return out\n",
    "\n",
    "class TransformerEncoderLayer:\n",
    "    def __init__(self, embed_size, num_heads, hidden_dim):\n",
    "        self.attention = MultiHeadSelfAttention(embed_size, num_heads)\n",
    "        self.attn_layer_norm = LayerNorm(embed_size)\n",
    "        \n",
    "        self.ffn = FeedForward(embed_size, hidden_dim)\n",
    "        self.ffn_layer_norm = LayerNorm(embed_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass for a single Transformer encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            X: Input of shape [batch_size, seq_length, embed_size]\n",
    "        \n",
    "        Returns:\n",
    "            Output of same shape as X\n",
    "        \"\"\"\n",
    "        # Multi-Head Self-Attention with residual connection and layer normalization\n",
    "        attn_output = self.attention.forward(X)  # [batch, seq, embed]\n",
    "        X = self.attn_layer_norm.forward(X + attn_output)  # [batch, seq, embed]\n",
    "        \n",
    "        # Feed-Forward Network with residual connection and layer normalization\n",
    "        ffn_output = self.ffn.forward(X)  # [batch, seq, embed]\n",
    "        X = self.ffn_layer_norm.forward(X + ffn_output)  # [batch, seq, embed]\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Initialize Output Layer weights and biases\n",
    "# [embed_size, vocab_size]\n",
    "W_out = np.random.randn(8, vocab_size) / np.sqrt(8)\n",
    "b_out = np.zeros(vocab_size)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"\n",
    "    Applies the softmax function to the input array along the specified axis.\n",
    "    \n",
    "    Args:\n",
    "        x: Input array.\n",
    "        axis: Axis along which to apply softmax.\n",
    "    \n",
    "    Returns:\n",
    "        Softmax applied array.\n",
    "    \"\"\"\n",
    "    shifted_x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(shifted_x)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(probs, targets):\n",
    "    \"\"\"\n",
    "    Computes the average cross-entropy loss over the batch.\n",
    "    \n",
    "    Args:\n",
    "        probs: Predicted probabilities of shape [batch_size, seq_length, vocab_size]\n",
    "        targets: True class indices of shape [batch_size, seq_length]\n",
    "    \n",
    "    Returns:\n",
    "        float: Average cross-entropy loss\n",
    "    \"\"\"\n",
    "    batch_size, seq_length, vocab_size = probs.shape\n",
    "    n = batch_size * seq_length\n",
    "    \n",
    "    # Ensure probabilities are clipped to avoid log(0)\n",
    "    epsilon = 1e-12\n",
    "    probs = np.clip(probs, epsilon, 1. - epsilon)\n",
    "    \n",
    "    # Flatten probabilities and targets\n",
    "    probs_flat = probs.reshape(n, vocab_size)\n",
    "    targets_flat = targets.flatten()\n",
    "    \n",
    "    # Compute log probabilities of the correct classes\n",
    "    log_probs = -np.log(probs_flat[np.arange(n), targets_flat])\n",
    "    \n",
    "    # Compute average loss\n",
    "    loss = np.sum(log_probs) / n\n",
    "    return loss\n",
    "\n",
    "def compute_accuracy(probs, targets):\n",
    "    \"\"\"\n",
    "    Computes the accuracy over the batch.\n",
    "    \n",
    "    Args:\n",
    "        probs: Predicted probabilities of shape [batch_size, seq_length, vocab_size]\n",
    "        targets: True class indices of shape [batch_size, seq_length]\n",
    "    \n",
    "    Returns:\n",
    "        float: Accuracy (between 0 and 1)\n",
    "    \"\"\"\n",
    "    predictions = np.argmax(probs, axis=-1)  # [batch_size, seq_length]\n",
    "    correct = (predictions == targets).astype(float)\n",
    "    accuracy = np.mean(correct)\n",
    "    return accuracy\n",
    "\n",
    "class TransformerLanguageModel_NumPy:\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers, seq_length):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Initialize Embedding Matrix\n",
    "        self.E = np.random.randn(vocab_size, embed_size) / np.sqrt(vocab_size)\n",
    "        \n",
    "        # Initialize Positional Encoding\n",
    "        self.positional_encoding = get_positional_encoding(seq_length, embed_size)  # [seq_length, embed_size]\n",
    "        \n",
    "        # Initialize Transformer Encoder Layers\n",
    "        self.encoder_layers = [TransformerEncoderLayer(embed_size, num_heads, hidden_dim) for _ in range(num_layers)]\n",
    "        \n",
    "        # Initialize Output Layer\n",
    "        self.W_out = np.random.randn(embed_size, vocab_size) / np.sqrt(embed_size)\n",
    "        self.b_out = np.zeros(vocab_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the entire Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequences of shape [batch_size, seq_length]\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, seq_length, vocab_size]\n",
    "            probs: [batch_size, seq_length, vocab_size]\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = X.shape\n",
    "        \n",
    "        # Token Embedding\n",
    "        X_emb = self.E[X]  # [batch_size, seq_length, embed_size]\n",
    "        \n",
    "        # Add Positional Encoding\n",
    "        X_emb += self.positional_encoding  # Broadcasting [seq_length, embed_size]\n",
    "        \n",
    "        # Pass through Transformer Encoder Layers\n",
    "        for layer in self.encoder_layers:\n",
    "            X_emb = layer.forward(X_emb)  # [batch_size, seq_length, embed_size]\n",
    "        \n",
    "        # Output Layer\n",
    "        logits = X_emb @ self.W_out + self.b_out  # [batch_size, seq_length, vocab_size]\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probs = softmax(logits, axis=-1)  # [batch_size, seq_length, vocab_size]\n",
    "        \n",
    "        return logits, probs\n",
    "    \n",
    "# Initialize model\n",
    "model = TransformerLanguageModel_NumPy(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_size=8,\n",
    "    num_heads=2,\n",
    "    hidden_dim=16,\n",
    "    num_layers=1,\n",
    "    seq_length=3\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Forward Pass\n",
    "    logits, probs = model.forward(X)  # [batch_size, seq_length, vocab_size]\n",
    "    \n",
    "    # Compute Loss\n",
    "    loss = cross_entropy_loss(probs, targets)\n",
    "    \n",
    "    # Compute Accuracy\n",
    "    accuracy = compute_accuracy(probs, targets)\n",
    "    \n",
    "    # Backward Pass (Mocked Gradients)\n",
    "    # Proper backpropagation is not implemented; thus, parameters are not effectively updated\n",
    "    # This mock serves only for structural demonstration\n",
    "    # To implement real training, gradients for all parameters must be computed accurately\n",
    "    \n",
    "    # Mocked gradient for output layer\n",
    "    grad_logits = probs.copy()  # [batch_size, seq_length, vocab_size]\n",
    "    grad_logits[np.arange(X.shape[0])[:, None], \n",
    "                np.arange(X.shape[1]), \n",
    "                targets] -= 1\n",
    "    grad_logits /= (X.shape[0] * X.shape[1])  # Normalize gradients\n",
    "    \n",
    "    # Compute gradients for output layer\n",
    "    X_emb = model.E[X] + model.positional_encoding  # [batch_size, seq_length, embed_size]\n",
    "    for layer in model.encoder_layers:\n",
    "        X_emb = layer.forward(X_emb)  # [batch_size, seq_length, embed_size]\n",
    "    grad_W_out = X_emb.reshape(-1, model.embed_size).T @ grad_logits.reshape(-1, vocab_size)  # [embed_size, vocab_size]\n",
    "    grad_b_out = np.sum(grad_logits, axis=(0,1))  # [vocab_size]\n",
    "    \n",
    "    # Update Output Layer Parameters\n",
    "    model.W_out -= learning_rate * grad_W_out\n",
    "    model.b_out -= learning_rate * grad_b_out\n",
    "    \n",
    "    # (No gradient computations for embedding or encoder layers)\n",
    "    \n",
    "    # Logging\n",
    "    if epoch % 100 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:4d}: Loss = {loss:.4f}, Accuracy = {accuracy*100:.2f}%\")\n",
    "\n",
    "def decode_predictions(logits, idx_to_word):\n",
    "    \"\"\"\n",
    "    Decodes logits to predicted token indices.\n",
    "    \n",
    "    Args:\n",
    "        logits: [batch_size, seq_length, vocab_size]\n",
    "        idx_to_word: Dictionary mapping indices to words\n",
    "    \n",
    "    Returns:\n",
    "        List of predicted token sequences.\n",
    "    \"\"\"\n",
    "    preds = np.argmax(logits, axis=-1)  # [batch_size, seq_length]\n",
    "    predicted_sequences = []\n",
    "    for seq in preds:\n",
    "        predicted_seq = [idx_to_word[idx] for idx in seq]\n",
    "        predicted_sequences.append(predicted_seq)\n",
    "    return predicted_sequences\n",
    "\n",
    "# Get predictions\n",
    "predicted_sequences = decode_predictions(logits, idx_to_word)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    input_seq = [idx_to_word[idx] for idx in X[i]]\n",
    "    target_seq = [idx_to_word[idx] for idx in targets[i]]\n",
    "    predicted_seq = predicted_sequences[i]\n",
    "    print(f\"\\nSequence {i+1}:\")\n",
    "    print(f\"Input:    {' '.join(input_seq)}\")\n",
    "    print(f\"Target:   {' '.join(target_seq)}\")\n",
    "    print(f\"Predicted:{' '.join(predicted_seq)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leetcode_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
