{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) Implementation from Scratch\n",
    "\n",
    "## Introduction to PCA\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction method used to transform a large set of variables into a smaller set that still contains most of the original information.\n",
    "\n",
    "## Steps of PCA Algorithm\n",
    "\n",
    "1. Standardize the dataset.\n",
    "2. Compute the covariance matrix of the standardized data.\n",
    "3. Calculate eigenvalues and eigenvectors from the covariance matrix.\n",
    "4. Sort eigenvalues and corresponding eigenvectors in descending order.\n",
    "5. Select the top k eigenvectors as principal components.\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "- Principal components represent directions in feature space along which the variance of the data is maximized.\n",
    "- PCA helps in dimensionality reduction and visualization of high-dimensional data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Components:\n",
      " [[0.7071]\n",
      " [0.7071]]\n",
      "[[ 0.6855  0.0776]\n",
      " [ 0.6202  0.4586]\n",
      " [-0.3814  0.8853]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "def pca(data, k):\n",
    "    \"\"\"\n",
    "    Perform PCA on the given data from scratch,\n",
    "    fully standardizing features (mean 0, std 1).\n",
    "    \n",
    "    Parameters:\n",
    "    - data (np.ndarray): shape (n_samples, n_features)\n",
    "    - k (int): number of principal components to return\n",
    "    \n",
    "    Returns:\n",
    "    - principal_components (np.ndarray): shape (n_features, k)\n",
    "    \"\"\"\n",
    "    # 1. Convert data to float, then mean-center and variance-scale (standardize)\n",
    "    data = data.astype(float)\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std_dev = np.std(data, axis=0)\n",
    "    standardized_data = (data - mean) / std_dev\n",
    "\n",
    "    # 2. Compute covariance matrix\n",
    "    covariance_matrix = np.cov(standardized_data, rowvar=False)\n",
    "\n",
    "    # 3. Eigen-decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "    # 4. Sort eigenvalues/eigenvectors in descending order of eigenvalues\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # 5. Keep top k eigenvectors\n",
    "    principal_components = eigenvectors[:, :k]\n",
    "\n",
    "    # 6. Fix signs for consistency (optional but helps match exact expected output)\n",
    "    #    Flip any eigenvector whose largest absolute-value entry is negative\n",
    "    for i in range(principal_components.shape[1]):\n",
    "        col = principal_components[:, i]\n",
    "        if col[np.argmax(np.abs(col))] < 0:\n",
    "            principal_components[:, i] *= -1\n",
    "\n",
    "    # 7. Round to 4 decimals\n",
    "    return np.round(principal_components, 4)\n",
    "# Example Usage\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "k = 1\n",
    "principal_components = pca(data, k)\n",
    "print(\"Principal Components:\\n\", principal_components)\n",
    "print(pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leetcode_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
