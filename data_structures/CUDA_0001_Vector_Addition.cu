/**
 *   âœ“ What a GPU is and why it exists
 *   âœ“ How parallel computing works
 *   âœ“ How to write your first CUDA program
 *   âœ“ What HPC (High Performance Computing) means
 * 
 * No prior experience needed - just curiosity! ğŸ§ 
 */

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CHAPTER 1: THE PIZZA SHOP ANALOGY ğŸ•
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
/**
 * 
 * Imagine you own a pizza shop and need to make 1000 pizzas for a big party.
 * 
 * 
 * OPTION A: THE CPU WAY (One Expert Chef)
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * 
 *     ğŸ‘¨â€ğŸ³ Master Chef (CPU)
 *     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 *     â”‚  â€¢ Makes pizzas ONE at a time               â”‚
 *     â”‚  â€¢ Very skilled - can make ANY type         â”‚
 *     â”‚  â€¢ Can handle complex recipes               â”‚
 *     â”‚  â€¢ Speed: 1 pizza per minute                â”‚
 *     â”‚                                             â”‚
 *     â”‚  Time for 1000 pizzas = 1000 minutes! ğŸ˜°    â”‚
 *     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 * 
 * 
 * OPTION B: THE GPU WAY (1000 Simple Workers)
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * 
 *     ğŸ‘·ğŸ‘·ğŸ‘·ğŸ‘·ğŸ‘·ğŸ‘·ğŸ‘·ğŸ‘·... (1000 workers = GPU threads)
 *     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 *     â”‚  â€¢ Each worker makes ONE pizza              â”‚
 *     â”‚  â€¢ Workers are simple - same recipe only    â”‚
 *     â”‚  â€¢ ALL workers work AT THE SAME TIME        â”‚
 *     â”‚                                             â”‚
 *     â”‚  Time for 1000 pizzas = 1 minute! ğŸ‰        â”‚
 *     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 * 
 * 
 * THIS IS THE KEY INSIGHT:
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * 
 *   CPU = Few powerful cores doing complex tasks SEQUENTIALLY
 *   GPU = THOUSANDS of simple cores doing easy tasks IN PARALLEL
 * 
 *   Real numbers:
 *   â€¢ Your laptop CPU: 4-16 cores
 *   â€¢ Gaming GPU: 2,000-16,000 cores!
 * 
 */

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CHAPTER 2: WHAT'S INSIDE YOUR COMPUTER? ğŸ’»
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
/**
 * 
 *     YOUR COMPUTER
 *     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 *     â”‚                                                                   â”‚
 *     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
 *     â”‚   â”‚      CPU        â”‚          â”‚           GPU               â”‚   â”‚
 *     â”‚   â”‚   "The Brain"   â”‚          â”‚      "The Army"             â”‚   â”‚
 *     â”‚   â”‚                 â”‚          â”‚                             â”‚   â”‚
 *     â”‚   â”‚  â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”   â”‚          â”‚  â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”  â”‚   â”‚
 *     â”‚   â”‚  â”‚ C â”‚ â”‚ C â”‚   â”‚          â”‚  â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜  â”‚   â”‚
 *     â”‚   â”‚  â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜   â”‚          â”‚  â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”  â”‚   â”‚
 *     â”‚   â”‚  â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”   â”‚          â”‚  â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜  â”‚   â”‚
 *     â”‚   â”‚  â”‚ C â”‚ â”‚ C â”‚   â”‚          â”‚  â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”â”Œâ”€â”  â”‚   â”‚
 *     â”‚   â”‚  â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜   â”‚          â”‚  â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜â””â”€â”˜  â”‚   â”‚
 *     â”‚   â”‚   4 big cores  â”‚          â”‚  ... thousands of tiny     â”‚   â”‚
 *     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚      cores!                â”‚   â”‚
 *     â”‚          â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
 *     â”‚          â”‚                                â”‚                      â”‚
 *     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
 *     â”‚   â”‚     RAM     â”‚               â”‚   Video RAM     â”‚             â”‚
 *     â”‚   â”‚  "Desk"     â”‚               â”‚   (VRAM)        â”‚             â”‚
 *     â”‚   â”‚  16-64 GB   â”‚               â”‚   8-24 GB       â”‚             â”‚
 *     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
 *     â”‚                                                                   â”‚
 *     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 * 
 * 
 * KEY TERMINOLOGY:
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * 
 *   HOST   = CPU + its RAM (where your normal programs run)
 *   DEVICE = GPU + its VRAM (where CUDA programs run)
 * 
 *   Think of it like:
 *   - HOST is your office
 *   - DEVICE is a factory you can send work to
 * 
 */

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CHAPTER 3: WHEN TO USE GPU? ğŸ¤”
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
/**
 * 
 * GPU is GREAT for:                    GPU is BAD for:
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * âœ“ Same operation on                  âœ— Complex logic with
 *   millions of items                    many if/else branches
 * 
 * âœ“ Video games (render                âœ— Tasks that depend on
 *   millions of pixels)                  previous results
 * 
 * âœ“ AI/Machine Learning                âœ— Small datasets
 *   (matrix math)                        (overhead > benefit)
 * 
 * âœ“ Image processing                   âœ— File I/O, networking
 *   (filter each pixel)                  (CPU does this)
 * 
 * âœ“ Scientific simulations             âœ— Sequential algorithms
 *   (weather, physics)                   (must be step-by-step)
 * 
 * 
 * THE GOLDEN RULE:
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * 
 *   "Is this problem EMBARRASSINGLY PARALLEL?"
 * 
 *   That means: Can I split this into thousands of INDEPENDENT tasks?
 * 
 *   Vector Addition: A[i] + B[i] for each i
 *   â””â”€â†’ YES! Each addition is independent. PERFECT for GPU! âœ“
 * 
 *   Fibonacci: F(n) = F(n-1) + F(n-2)  
 *   â””â”€â†’ NO! Each number depends on previous ones. âœ—
 * 
 */

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CHAPTER 4: CUDA - TEACHING THE GPU ğŸ“š
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
/**
 * 
 * CUDA = "Compute Unified Device Architecture"
 *      = NVIDIA's language to program GPUs
 *      = C/C++ with some special keywords
 * 
 * 
 * THE THREE SPECIAL KEYWORDS:
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * 
 *   __global__  = "This function runs ON the GPU, called FROM the CPU"
 *                 This is called a KERNEL
 * 
 *   __device__  = "This function runs ON the GPU, called FROM the GPU"
 *                 Helper functions for kernels
 * 
 *   __host__    = "This function runs ON the CPU" (normal function)
 *                 This is the default
 * 
 * 
 * EXAMPLE:
 * â•â•â•â•â•â•â•â•
 * 
 *   // This runs on CPU (normal C++)
 *   void cpuFunction() {
 *       printf("I'm on the CPU!\n");
 *   }
 * 
 *   // This runs on GPU (CUDA kernel)
 *   __global__ void gpuFunction() {
 *       printf("I'm on the GPU!\n");
 *   }
 * 
 */

 #include <stdio.h>
 #include <cuda_runtime.h>
 
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 // CHAPTER 5: YOUR FIRST KERNEL - "Hello, I am Thread #X!" ğŸ‘‹
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 
 // This is a KERNEL - a function that runs on the GPU
 __global__ void helloFromGPU() {
     // Every thread knows its own ID number!
     int myID = threadIdx.x;
     printf("Hello! I am thread #%d running on the GPU!\n", myID);
 }
 
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 // CHAPTER 6: THREADS, BLOCKS, AND GRIDS ğŸ§±
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 /**
  * 
  * GPU organizes threads into a HIERARCHY (like a school!):
  * 
  * 
  *     GRID (The whole school)
  *     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  *     â”‚                                                          â”‚
  *     â”‚   BLOCK 0          BLOCK 1          BLOCK 2              â”‚
  *     â”‚   (Classroom 0)    (Classroom 1)    (Classroom 2)        â”‚
  *     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
  *     â”‚   â”‚ T0 T1 T2 â”‚     â”‚ T0 T1 T2 â”‚     â”‚ T0 T1 T2 â”‚         â”‚
  *     â”‚   â”‚ T3 T4 T5 â”‚     â”‚ T3 T4 T5 â”‚     â”‚ T3 T4 T5 â”‚         â”‚
  *     â”‚   â”‚ T6 T7 T8 â”‚     â”‚ T6 T7 T8 â”‚     â”‚ T6 T7 T8 â”‚   ...   â”‚
  *     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
  *     â”‚    (9 threads)      (9 threads)      (9 threads)         â”‚
  *     â”‚                                                          â”‚
  *     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  * 
  * 
  * VOCABULARY:
  * â•â•â•â•â•â•â•â•â•â•â•
  * 
  *   threadIdx.x = Thread's seat number within its classroom (0, 1, 2, ...)
  *   blockIdx.x  = Which classroom? (0, 1, 2, ...)
  *   blockDim.x  = How many seats per classroom?
  *   gridDim.x   = How many classrooms total?
  * 
  * 
  * GLOBAL ID (Unique across entire school):
  * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  * 
  *   globalID = blockIdx.x * blockDim.x + threadIdx.x
  * 
  *   Example: 3 classrooms, 10 seats each
  *   
  *   Classroom 0: threads 0-9   (0*10+0 to 0*10+9)
  *   Classroom 1: threads 10-19 (1*10+0 to 1*10+9)  
  *   Classroom 2: threads 20-29 (2*10+0 to 2*10+9)
  * 
  */
 
 // Let's visualize this!
 __global__ void showThreadInfo() {
     int globalID = blockIdx.x * blockDim.x + threadIdx.x;
     
     printf("I am Thread %d in Block %d â†’ My Global ID is %d\n",
            threadIdx.x, blockIdx.x, globalID);
 }
 
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 // CHAPTER 7: VECTOR ADDITION - THE COMPLETE EXAMPLE ğŸ¯
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 /**
  * 
  * PROBLEM: Add two arrays element by element
  * 
  *     A = [1, 2, 3, 4, 5, 6, 7, 8]
  *   + B = [10,20,30,40,50,60,70,80]
  *   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  *     C = [11,22,33,44,55,66,77,88]
  * 
  * 
  * CPU WAY (Sequential):
  * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  *   for (i = 0; i < 8; i++) {
  *       C[i] = A[i] + B[i];  // Do one at a time
  *   }
  *   Time: 8 steps
  * 
  * 
  * GPU WAY (Parallel):
  * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  *   Launch 8 threads, each does ONE addition:
  * 
  *   Thread 0: C[0] = A[0] + B[0] = 1 + 10 = 11  â”€â”
  *   Thread 1: C[1] = A[1] + B[1] = 2 + 20 = 22   â”‚
  *   Thread 2: C[2] = A[2] + B[2] = 3 + 30 = 33   â”‚
  *   Thread 3: C[3] = A[3] + B[3] = 4 + 40 = 44   â”œâ”€ ALL AT SAME TIME!
  *   Thread 4: C[4] = A[4] + B[4] = 5 + 50 = 55   â”‚
  *   Thread 5: C[5] = A[5] + B[5] = 6 + 60 = 66   â”‚
  *   Thread 6: C[6] = A[6] + B[6] = 7 + 70 = 77   â”‚
  *   Thread 7: C[7] = A[7] + B[7] = 8 + 80 = 88  â”€â”˜
  *   
  *   Time: 1 step! (8x faster)
  * 
  */
 
 // THE KERNEL (runs on GPU)
 __global__ void vectorAdd(float *A, float *B, float *C, int N) {
     // Step 1: Figure out which element I'm responsible for
     int i = blockIdx.x * blockDim.x + threadIdx.x;
     
     // Step 2: Make sure I don't go out of bounds!
     //         (We might launch more threads than elements)
     if (i < N) {
         // Step 3: Do my ONE job - add my element
         C[i] = A[i] + B[i];
     }
 }
 
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 // CHAPTER 8: THE COMPLETE WORKFLOW ğŸ”„
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 /**
  * 
  * THE 5-STEP GPU WORKFLOW:
  * 
  *     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  *     â”‚                                                                 â”‚
  *     â”‚  STEP 1: Allocate memory on GPU                                â”‚
  *     â”‚          cudaMalloc(&d_A, size);                               â”‚
  *     â”‚                                                                 â”‚
  *     â”‚              CPU                      GPU                       â”‚
  *     â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
  *     â”‚          â”‚ h_A[8]  â”‚              â”‚ d_A[8]  â”‚ â† empty          â”‚
  *     â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
  *     â”‚                                                                 â”‚
  *     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  *                                   â”‚
  *                                   â–¼
  *     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  *     â”‚                                                                 â”‚
  *     â”‚  STEP 2: Copy data FROM CPU TO GPU                             â”‚
  *     â”‚          cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);   â”‚
  *     â”‚                                                                 â”‚
  *     â”‚              CPU          â•â•â•â•â•â•â–º      GPU                      â”‚
  *     â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
  *     â”‚          â”‚ 1,2,3...â”‚   COPY â†’     â”‚ 1,2,3...â”‚                  â”‚
  *     â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
  *     â”‚                                                                 â”‚
  *     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  *                                   â”‚
  *                                   â–¼
  *     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  *     â”‚                                                                 â”‚
  *     â”‚  STEP 3: Launch the kernel (GPU does the work!)                â”‚
  *     â”‚          vectorAdd<<<blocks, threads>>>(d_A, d_B, d_C, N);     â”‚
  *     â”‚                                                                 â”‚
  *     â”‚              CPU                      GPU                       â”‚
  *     â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
  *     â”‚          â”‚ waiting â”‚              â”‚ WORKING â”‚ â† parallel!      â”‚
  *     â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
  *     â”‚                                                                 â”‚
  *     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  *                                   â”‚
  *                                   â–¼
  *     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  *     â”‚                                                                 â”‚
  *     â”‚  STEP 4: Copy results FROM GPU TO CPU                          â”‚
  *     â”‚          cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);   â”‚
  *     â”‚                                                                 â”‚
  *     â”‚              CPU          â—„â•â•â•â•â•â•      GPU                      â”‚
  *     â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
  *     â”‚          â”‚ 11,22...â”‚   â† COPY     â”‚ 11,22...â”‚                  â”‚
  *     â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
  *     â”‚                                                                 â”‚
  *     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  *                                   â”‚
  *                                   â–¼
  *     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  *     â”‚                                                                 â”‚
  *     â”‚  STEP 5: Free GPU memory (cleanup!)                            â”‚
  *     â”‚          cudaFree(d_A);                                        â”‚
  *     â”‚                                                                 â”‚
  *     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  * 
  */
 
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 // CHAPTER 9: COMPLETE WORKING CODE! ğŸ’»
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 
 // Error checking helper (like a safety net)
 #define CHECK_CUDA(call) { \
     cudaError_t err = call; \
     if (err != cudaSuccess) { \
         printf("âŒ CUDA Error: %s\n", cudaGetErrorString(err)); \
         exit(1); \
     } \
 }
 
 int main() {
     printf("\n");
     printf("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n");
     printf("â•‘     ğŸ® GPU COMPUTING 101 - INTERACTIVE TUTORIAL ğŸ®           â•‘\n");
     printf("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n");
 
     // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     // DEMO 1: Hello from GPU threads!
     // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
     printf("DEMO 1: Launching 5 threads on the GPU\n");
     printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
     
     // Launch 1 block with 5 threads
     // Syntax: kernel<<<numBlocks, threadsPerBlock>>>()
     helloFromGPU<<<1, 5>>>();
     cudaDeviceSynchronize();  // Wait for GPU to finish
     printf("\n");
 
     // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     // DEMO 2: Understanding Blocks and Threads
     // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
     printf("DEMO 2: 3 Blocks Ã— 4 Threads = 12 Total Threads\n");
     printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
     
     // Launch 3 blocks, each with 4 threads
     showThreadInfo<<<3, 4>>>();
     cudaDeviceSynchronize();
     printf("\n");
 
     // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     // DEMO 3: Vector Addition (The Main Event!)
     // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
     printf("DEMO 3: VECTOR ADDITION - Adding 1 Million Numbers!\n");
     printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
     
     // Problem size: 1 million elements
     int N = 1000000;
     size_t size = N * sizeof(float);
     
     printf("ğŸ“Š Problem size: %d elements (%.2f MB per array)\n", N, size/1e6);
     
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     // STEP 1: Allocate CPU memory (host)
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     printf("\nğŸ“Œ Step 1: Allocating CPU memory...\n");
     float *h_A = (float*)malloc(size);  // h_ = host (CPU)
     float *h_B = (float*)malloc(size);
     float *h_C = (float*)malloc(size);
     
     // Fill with test data
     for (int i = 0; i < N; i++) {
         h_A[i] = i;           // A = [0, 1, 2, 3, ...]
         h_B[i] = i * 2;       // B = [0, 2, 4, 6, ...]
     }
     printf("   âœ“ Created arrays A and B with test data\n");
     printf("   Example: A[0]=%0.f, A[1]=%.0f, A[2]=%.0f\n", h_A[0], h_A[1], h_A[2]);
     printf("   Example: B[0]=%0.f, B[1]=%.0f, B[2]=%.0f\n", h_B[0], h_B[1], h_B[2]);
     
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     // STEP 2: Allocate GPU memory (device)
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     printf("\nğŸ“Œ Step 2: Allocating GPU memory...\n");
     float *d_A, *d_B, *d_C;  // d_ = device (GPU)
     
     CHECK_CUDA(cudaMalloc(&d_A, size));
     CHECK_CUDA(cudaMalloc(&d_B, size));
     CHECK_CUDA(cudaMalloc(&d_C, size));
     printf("   âœ“ Allocated %.2f MB on GPU\n", 3*size/1e6);
     
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     // STEP 3: Copy data CPU â†’ GPU
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     printf("\nğŸ“Œ Step 3: Copying data from CPU to GPU...\n");
     CHECK_CUDA(cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice));
     CHECK_CUDA(cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice));
     printf("   âœ“ Data transferred to GPU!\n");
     
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     // STEP 4: Launch the kernel! ğŸš€
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     printf("\nğŸ“Œ Step 4: Launching GPU kernel...\n");
     
     // Configure launch parameters
     int threadsPerBlock = 256;  // Common choice (multiple of 32)
     int numBlocks = (N + threadsPerBlock - 1) / threadsPerBlock;
     
     printf("   Configuration:\n");
     printf("   â€¢ Threads per block: %d\n", threadsPerBlock);
     printf("   â€¢ Number of blocks: %d\n", numBlocks);
     printf("   â€¢ Total threads: %d\n", threadsPerBlock * numBlocks);
     
     // Time the kernel
     cudaEvent_t start, stop;
     cudaEventCreate(&start);
     cudaEventCreate(&stop);
     
     cudaEventRecord(start);
     
     // ğŸš€ LAUNCH THE KERNEL! ğŸš€
     vectorAdd<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, N);
     
     cudaEventRecord(stop);
     cudaEventSynchronize(stop);
     
     float milliseconds = 0;
     cudaEventElapsedTime(&milliseconds, start, stop);
     
     printf("   âœ“ Kernel finished in %.4f ms\n", milliseconds);
     printf("   âš¡ Speed: %.2f billion additions per second!\n", 
            (N / milliseconds) / 1e6);
     
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     // STEP 5: Copy results GPU â†’ CPU
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     printf("\nğŸ“Œ Step 5: Copying results back to CPU...\n");
     CHECK_CUDA(cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost));
     printf("   âœ“ Results retrieved!\n");
     
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     // STEP 6: Verify results
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     printf("\nğŸ“Œ Step 6: Verifying results...\n");
     
     // Check first 5 results
     printf("   Sample results:\n");
     for (int i = 0; i < 5; i++) {
         printf("   A[%d] + B[%d] = %.0f + %.0f = %.0f âœ“\n", 
                i, i, h_A[i], h_B[i], h_C[i]);
     }
     
     // Verify all results
     int errors = 0;
     for (int i = 0; i < N; i++) {
         if (h_C[i] != h_A[i] + h_B[i]) errors++;
     }
     
     if (errors == 0) {
         printf("\n   ğŸ‰ SUCCESS! All %d calculations correct!\n", N);
     } else {
         printf("\n   âŒ Found %d errors\n", errors);
     }
     
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     // STEP 7: Cleanup
     // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     printf("\nğŸ“Œ Step 7: Cleaning up...\n");
     cudaFree(d_A);
     cudaFree(d_B);
     cudaFree(d_C);
     free(h_A);
     free(h_B);
     free(h_C);
     cudaEventDestroy(start);
     cudaEventDestroy(stop);
     printf("   âœ“ Memory freed!\n");
     
     // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     // GPU INFO
     // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     printf("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
     printf("YOUR GPU INFO:\n");
     printf("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
     
     cudaDeviceProp prop;
     cudaGetDeviceProperties(&prop, 0);
     
     printf("   Name: %s\n", prop.name);
     printf("   CUDA Cores: ~%d (estimated)\n", 
            prop.multiProcessorCount * 128);  // Approximate
     printf("   Memory: %.2f GB\n", prop.totalGlobalMem / 1e9);
     printf("   Max threads per block: %d\n", prop.maxThreadsPerBlock);
     printf("   Warp size: %d\n", prop.warpSize);
     
     printf("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n");
     printf("â•‘              ğŸ“ TUTORIAL COMPLETE! ğŸ“                         â•‘\n");
     printf("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n");
     
     return 0;
 }
 
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 // CHAPTER 10: WHAT IS HPC? (High Performance Computing) ğŸ¢
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 /**
  * 
  * HPC = Using MANY powerful computers together to solve HUGE problems
  * 
  * 
  * HIERARCHY OF COMPUTING POWER:
  * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  * 
  *   Your Laptop        â†’   1 CPU, maybe 1 GPU
  *         â†“
  *   Gaming Desktop     â†’   1 CPU, 1-2 powerful GPUs  
  *         â†“
  *   Workstation        â†’   1-2 CPUs, 1-4 GPUs (what researchers use)
  *         â†“
  *   Server             â†’   2-8 CPUs, 4-8 GPUs (in data centers)
  *         â†“
  *   Cluster            â†’   Many servers connected by fast network
  *         â†“
  *   SUPERCOMPUTER      â†’   THOUSANDS of servers, MILLIONS of cores!
  * 
  * 
  * REAL SUPERCOMPUTERS (2024):
  * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  * 
  *   #1 Frontier (USA)     - 8.7 million cores, 37,000 GPUs
  *                         - Can do 1 QUINTILLION calculations/second!
  *   
  *   #2 Aurora (USA)       - 60,000+ GPUs
  *   
  *   Used for: Climate modeling, drug discovery, AI training
  * 
  * 
  * HPC WORKFLOW:
  * â•â•â•â•â•â•â•â•â•â•â•â•â•
  * 
  *     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  *     â”‚  1. Write your code (CUDA, MPI, OpenMP)                   â”‚
  *     â”‚                     â†“                                      â”‚
  *     â”‚  2. Submit job to cluster queue                           â”‚
  *     â”‚                     â†“                                      â”‚
  *     â”‚  3. Scheduler assigns your job to available nodes         â”‚
  *     â”‚                     â†“                                      â”‚
  *     â”‚  4. Your code runs on hundreds/thousands of GPUs!         â”‚
  *     â”‚                     â†“                                      â”‚
  *     â”‚  5. Results saved to shared storage                       â”‚
  *     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  * 
  * 
  * COMMON HPC TOOLS:
  * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  * 
  *   CUDA      - Program a single GPU (what we learned!)
  *   OpenMP    - Parallelize across CPU cores
  *   MPI       - Distribute work across multiple computers
  *   Slurm     - Job scheduler for clusters
  *   
  * 
  */
 
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 // CHAPTER 11: SUMMARY - KEY TAKEAWAYS ğŸ“
 // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 /**
  * 
  * ğŸ¯ WHAT LEARNED TODAY:
  * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  * 
  * 1. CPU vs GPU
  *    â€¢ CPU: Few powerful cores, good for complex sequential tasks
  *    â€¢ GPU: Thousands of simple cores, good for parallel tasks
  * 
  * 2. CUDA Basics
  *    â€¢ __global__ = function that runs on GPU
  *    â€¢ Kernel = the GPU function
  *    â€¢ Thread = one worker on the GPU
  *    â€¢ Block = group of threads that can cooperate
  *    â€¢ Grid = all blocks launched by one kernel
  * 
  * 3. Thread Indexing
  *    â€¢ globalID = blockIdx.x * blockDim.x + threadIdx.x
  *    â€¢ Always check bounds: if (i < N)
  * 
  * 4. GPU Workflow
  *    â€¢ cudaMalloc() - allocate GPU memory
  *    â€¢ cudaMemcpy() - transfer data
  *    â€¢ kernel<<<blocks, threads>>>() - run on GPU
  *    â€¢ cudaFree() - cleanup
  * 
  * 5. HPC
  *    â€¢ Many computers working together
  *    â€¢ CUDA is foundation for GPU programming in HPC
  * 
  * 
  * ğŸš€ NEXT STEPS TO LEARN:
  * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  * 
  * Beginner:
  *   â–¡ Practice more vector operations (subtraction, multiplication)
  *   â–¡ Try 2D grids (for image processing)
  *   â–¡ Learn about shared memory
  * 
  * Intermediate:
  *   â–¡ Matrix multiplication
  *   â–¡ Reduction operations (sum all elements)
  *   â–¡ Memory coalescing optimization
  * 
  * Advanced:
  *   â–¡ Tensor cores and mixed precision
  *   â–¡ Multi-GPU programming
  *   â–¡ CUDA streams for async execution
  * 
  * 
  * ğŸ“š RESOURCES:
  * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  * 
  *   â€¢ NVIDIA CUDA Toolkit Documentation
  *   â€¢ leetgpu.com - Practice problems
  *   â€¢ "Programming Massively Parallel Processors" - Textbook
  *   â€¢ NVIDIA Developer Blog
  * 
  */

// nvcc -g -G -O0 CUDA_0001_Vector_Addition.cu -o test
// g++ -std=c++17 xxx.cpp -g -O0 -o test
