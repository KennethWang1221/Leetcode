{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML - LLMs - Evaluation - Perplexity\n",
    "\n",
    "Perplexity is a metric used to evaluate how well a probability model predicts a sample. It is calculated as the exponential of the cross-entropy, which reflects the model's uncertainty in predicting the next word.\n",
    "\n",
    "The formula for perplexity is:\n",
    " \n",
    "$ \\text{Perplexity} = \\exp\\left(H(P)\\right) $\n",
    "\n",
    "By substituting the cross-entropy formula, perplexity can also be expressed as:\n",
    "\n",
    "$ \\text{Perplexity} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(x_i)\\right) $\n",
    "\n",
    "In essence, perplexity is a transformation of the cross-entropy loss, expressing the error in terms of the effective number of choices (or \"confusion\") the model faces. A lower cross-entropy corresponds to a lower perplexity, indicating better model predictions.\n",
    "\n",
    "To compute perplexity:\n",
    "1. Compute the softmax probabilities.\n",
    "2. Calculate the cross-entropy loss for each sample in the batch.\n",
    "3. Average the losses.\n",
    "4. Compute the perplexity over the batch.\n",
    "\n",
    "Learn more: [Perplexity Explanation](https://chatgpt.com/share/67fad511-d220-8009-a7b1-060df0840166)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      " [[2.  1.  0.1]\n",
      " [1.5 0.5 0. ]\n",
      " [0.2 1.2 0.5]]\n",
      "True Labels (one-hot):\n",
      " [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Perplexity over the Batch: 2.909916162855865\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_softmax(logits):\n",
    "    \"\"\"\n",
    "    Compute the softmax probabilities for each row in the logits matrix.\n",
    "\n",
    "    Parameters:\n",
    "        logits (np.ndarray): A 2D array of shape (n_samples, n_classes)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A 2D array of the same shape with softmax probabilities.\n",
    "    \"\"\"\n",
    "    # Exponentiate the logits\n",
    "    exp_logits = np.exp(logits)\n",
    "    # Sum along the classes (axis=1) and maintain dimensions for broadcasting\n",
    "    sum_exp = np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    # Divide each exponential by the sum of exponentials for that sample\n",
    "    softmax_probs = exp_logits / sum_exp\n",
    "    return softmax_probs\n",
    "\n",
    "def compute_cross_entropy_loss(softmax_probs, true_labels):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss for each sample.\n",
    "\n",
    "    Parameters:\n",
    "        softmax_probs (np.ndarray): Predicted probabilities with shape (n_samples, n_classes)\n",
    "        true_labels (np.ndarray): One-hot encoded true labels with shape (n_samples, n_classes)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A 1D array containing the cross-entropy loss for each sample.\n",
    "    \"\"\"\n",
    "    # Use a small epsilon value to avoid log(0)\n",
    "    epsilon = 1e-12\n",
    "    # Compute the loss: -sum(y * log(probabilities)) for each sample\n",
    "    losses = -np.sum(true_labels * np.log(softmax_probs + epsilon), axis=1)\n",
    "    return losses\n",
    "\n",
    "def calculate_perplexity(logits, true_labels):\n",
    "    \"\"\"\n",
    "    Calculate the perplexity given logits and one-hot encoded true labels.\n",
    "    \n",
    "    Steps:\n",
    "    1. Compute softmax probabilities from logits.\n",
    "    2. Calculate the cross-entropy loss for each sample.\n",
    "    3. Average the loss across the batch.\n",
    "    4. Compute perplexity as the exponential of the average loss.\n",
    "    \n",
    "    Parameters:\n",
    "        logits (np.ndarray): A 2D array of shape (n_samples, n_classes)\n",
    "        true_labels (np.ndarray): One-hot encoded true labels of the same shape as logits.\n",
    "        \n",
    "    Returns:\n",
    "        float: The perplexity computed over the batch.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute softmax probabilities\n",
    "    softmax_probs = compute_softmax(logits)\n",
    "    \n",
    "    # Step 2: Compute cross-entropy loss per sample\n",
    "    losses = compute_cross_entropy_loss(softmax_probs, true_labels)\n",
    "    \n",
    "    # Step 3: Compute the average cross-entropy loss over the batch\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    # Step 4: Calculate perplexity as the exponential of the average loss\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage of the functions:\n",
    "# -----------------------------\n",
    "\n",
    "# Step 1: Define Batch Logits and True Labels\n",
    "# Assume we have a batch of 3 examples, each with logits for 3 classes.\n",
    "logits = np.array([\n",
    "    [2.0, 1.0, 0.1],   # Sample 1\n",
    "    [1.5, 0.5, 0.0],   # Sample 2\n",
    "    [0.2, 1.2, 0.5]    # Sample 3\n",
    "])\n",
    "print(\"Logits:\\n\", logits)\n",
    "\n",
    "# True labels in one-hot encoded form for each sample.\n",
    "true_labels = np.array([\n",
    "    [1, 0, 0],  # Sample 1: true class is Class 0\n",
    "    [0, 1, 0],  # Sample 2: true class is Class 1\n",
    "    [0, 0, 1]   # Sample 3: true class is Class 2\n",
    "])\n",
    "print(\"True Labels (one-hot):\\n\", true_labels)\n",
    "\n",
    "# Calculate perplexity from logits and true labels\n",
    "perplexity = calculate_perplexity(logits, true_labels)\n",
    "print(\"Perplexity over the Batch:\", perplexity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leetcode_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
