{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML - LLMs - Evaluation - Perplexity\n",
    "\n",
    "Perplexity is a metric used to evaluate how well a probability model predicts a sample. It is calculated as the exponential of the cross-entropy, which reflects the model's uncertainty in predicting the next word.\n",
    "\n",
    "The formula for perplexity is:\n",
    " \n",
    "$ \\text{Perplexity} = \\exp\\left(H(P)\\right) $\n",
    "\n",
    "By substituting the cross-entropy formula, perplexity can also be expressed as:\n",
    "\n",
    "$ \\text{Perplexity} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(x_i)\\right) $\n",
    "\n",
    "In essence, perplexity is a transformation of the cross-entropy loss, expressing the error in terms of the effective number of choices (or \"confusion\") the model faces. A lower cross-entropy corresponds to a lower perplexity, indicating better model predictions.\n",
    "\n",
    "To compute perplexity:\n",
    "1. Compute the softmax probabilities.\n",
    "2. Calculate the cross-entropy loss for each sample in the batch.\n",
    "3. Average the losses.\n",
    "4. Compute the perplexity over the batch.\n",
    "\n",
    "Learn more: [Perplexity Explanation](https://chatgpt.com/share/67fad511-d220-8009-a7b1-060df0840166)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      " [[2.  1.  0.1]\n",
      " [1.5 0.5 0. ]\n",
      " [0.2 1.2 0.5]]\n",
      "True Labels (one-hot):\n",
      " [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Perplexity over the Batch: 2.909916162855865\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_softmax(logits):\n",
    "    \"\"\"\n",
    "    Compute the softmax probabilities for each row in the logits matrix.\n",
    "\n",
    "    Parameters:\n",
    "        logits (np.ndarray): A 2D array of shape (n_samples, n_classes)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A 2D array of the same shape with softmax probabilities.\n",
    "    \"\"\"\n",
    "    # Exponentiate the logits\n",
    "    # Subtract the max value for numerical stability.\n",
    "    logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(logits_stable)\n",
    "    # Sum along the classes (axis=1) and maintain dimensions for broadcasting\n",
    "    sum_exp = np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    # Divide each exponential by the sum of exponentials for that sample\n",
    "    softmax_probs = exp_logits / sum_exp\n",
    "    return softmax_probs\n",
    "\n",
    "def compute_cross_entropy_loss(softmax_probs, true_labels):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss for each sample using one-hot encoded true labels.\n",
    "    \n",
    "    In this implementation:\n",
    "      - `softmax_probs` is a 2D numpy array of shape (n_samples, n_classes),\n",
    "      - `true_labels` is a 2D numpy array of one-hot encoded labels of shape (n_samples, n_classes).\n",
    "      \n",
    "    The loss is computed as:\n",
    "         loss = - sum( y * log(probabilities) )\n",
    "    where the summation is taken over the classes.\n",
    "    \n",
    "    Because the true labels are one-hot encoded, all elements except the one corresponding to the true class are zero.\n",
    "    Summing along the classes picks out the negative log probability for the true class.\n",
    "    \n",
    "    Parameters:\n",
    "        softmax_probs (np.ndarray): Predicted probabilities with shape (n_samples, n_classes)\n",
    "        true_labels (np.ndarray): One-hot encoded true labels with shape (n_samples, n_classes)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A 1D array containing the cross-entropy loss for each sample.\n",
    "    \"\"\"\n",
    "    # Use a small epsilon value to avoid log(0)\n",
    "    # For each sample, multiply element-wise with the one-hot encoded true labels.\n",
    "    # This zeroes out contributions from all classes except the true one.\n",
    "    # Sum along the classes gives the negative log probability of the true class.\n",
    "    epsilon = 1e-12\n",
    "    # Compute the loss: -sum(y * log(probabilities)) for each sample\n",
    "    losses = -np.sum(true_labels * np.log(softmax_probs + epsilon), axis=1)\n",
    "\n",
    "    # Step 3: Compute the average cross-entropy loss over the batch\n",
    "    avg_loss = np.mean(losses)\n",
    "    return avg_loss\n",
    "\n",
    "def calculate_perplexity(logits, true_labels):\n",
    "    \"\"\"\n",
    "    Calculate the perplexity given logits and one-hot encoded true labels.\n",
    "    \n",
    "    Steps:\n",
    "    1. Compute softmax probabilities from logits.\n",
    "    2. Calculate the cross-entropy loss for each sample.\n",
    "    3. Average the loss across the batch.\n",
    "    4. Compute perplexity as the exponential of the average loss.\n",
    "    \n",
    "    Parameters:\n",
    "        logits (np.ndarray): A 2D array of shape (n_samples, n_classes)\n",
    "        true_labels (np.ndarray): One-hot encoded true labels of the same shape as logits.\n",
    "        \n",
    "    Returns:\n",
    "        float: The perplexity computed over the batch.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute softmax probabilities\n",
    "    softmax_probs = compute_softmax(logits)\n",
    "    \n",
    "    # Step 2 and 3: Compute cross-entropy loss per sample\n",
    "    avg_loss = compute_cross_entropy_loss(softmax_probs, true_labels)\n",
    "    \n",
    "    # Step 4: Calculate perplexity as the exponential of the average loss\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage of the functions:\n",
    "# -----------------------------\n",
    "\n",
    "# Step 1: Define Batch Logits and True Labels\n",
    "# Assume we have a batch of 3 examples, each with logits for 3 classes.\n",
    "logits = np.array([\n",
    "    [2.0, 1.0, 0.1],   # Sample 1\n",
    "    [1.5, 0.5, 0.0],   # Sample 2\n",
    "    [0.2, 1.2, 0.5]    # Sample 3\n",
    "])\n",
    "print(\"Logits:\\n\", logits)\n",
    "\n",
    "# True labels in one-hot encoded form for each sample.\n",
    "true_labels = np.array([\n",
    "    [1, 0, 0],  # Sample 1: true class is Class 0\n",
    "    [0, 1, 0],  # Sample 2: true class is Class 1\n",
    "    [0, 0, 1]   # Sample 3: true class is Class 2\n",
    "])\n",
    "print(\"True Labels (one-hot):\\n\", true_labels)\n",
    "\n",
    "# Calculate perplexity from logits and true labels\n",
    "perplexity = calculate_perplexity(logits, true_labels)\n",
    "print(\"Perplexity over the Batch:\", perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML - LLMs - Evaluation - Perplexity (Logits from real model prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from fractions import Fraction\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from fractions import Fraction\n",
    "# For embedding-based retrieval\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_perplexity(text, manually=True):\n",
    "    \"\"\"\n",
    "    Compute the perplexity for a given text using the model.\n",
    "    \n",
    "    There are two modes controlled by the 'manually' flag:\n",
    "    - If manually=True: the function computes the perplexity manually by:\n",
    "        1. Encoding the text and obtaining logits.\n",
    "        2. Shifting the logits and true labels so that we predict token t from tokens < t.\n",
    "        3. Manually computing softmax probabilities, token-level cross-entropy loss,\n",
    "            averaging them, and computing perplexity as exp(average loss).\n",
    "    - If manually=False: the function computes perplexity using the model's built-in loss,\n",
    "        which automatically handles shifting.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "        manually (bool): If True, use manual calculation; if False, use the model's loss.\n",
    "    \n",
    "    Returns:\n",
    "        float: The computed perplexity.\n",
    "    \"\"\"\n",
    "\n",
    "    pp_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B-Instruct')\n",
    "    pp_model = AutoModelForCausalLM.from_pretrained(\n",
    "        'meta-llama/Llama-3.2-1B-Instruct',\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    pp_model.to(\"cuda\")\n",
    "    pp_model.eval()\n",
    "\n",
    "    # Encode the text.\n",
    "    input_ids = pp_tokenizer.encode(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    if manually:\n",
    "        ignore_index = -100\n",
    "\n",
    "        # Step 2: Forward pass to obtain logits.\n",
    "        with torch.no_grad():\n",
    "            outputs = pp_model(input_ids)\n",
    "            logits = outputs.logits  # Shape: [batch, seq_len, vocab_size]\n",
    "        logits = logits.float()\n",
    "        vocab_size = logits.size(-1)\n",
    "        \n",
    "        # Step 3: Pad the input IDs with the ignore index so that we can shift them.\n",
    "        # Padding on the right will result in a tensor of shape [batch, seq_len+1].\n",
    "        padded_labels = F.pad(input_ids, (0, 1), value=ignore_index)\n",
    "        \n",
    "        # Shift the labels: Remove the first token so that targets become tokens 1...L.\n",
    "        shifted_true_labels = padded_labels[:, 1:].contiguous()  # Shape: [batch, seq_len]\n",
    "        \n",
    "        # Step 4: Align logits with the shifted labels.\n",
    "        shifted_logits = logits[:, :shifted_true_labels.size(1), :]  # Shape: [batch, seq_len, vocab_size]\n",
    "        \n",
    "        # Step 5: Manually compute softmax probabilities.\n",
    "        # For numerical stability, subtract the max logit in each vocabulary slice.\n",
    "        max_logits, _ = torch.max(shifted_logits, dim=-1, keepdim=True)\n",
    "        stable_logits = shifted_logits - max_logits\n",
    "        exp_logits = torch.exp(stable_logits)\n",
    "        sum_exp = torch.sum(exp_logits, dim=-1, keepdim=True)\n",
    "        softmax_probs = exp_logits / sum_exp  # Shape: [batch, seq_len, vocab_size]\n",
    "        \n",
    "        # Step 6: Prepare valid mask and adjusted labels.\n",
    "        # Create a mask of valid tokens (those not equal to ignore_index).\n",
    "        valid_mask = (shifted_true_labels != ignore_index)\n",
    "        # Create a copy of shifted_true_labels and replace ignore_index values with a dummy index (e.g., 0).\n",
    "        adjusted_labels = shifted_true_labels.clone()\n",
    "        adjusted_labels[~valid_mask] = 0\n",
    "        \n",
    "        # Step 7: Gather the probabilities for the true (shifted) labels.\n",
    "        # Since adjusted_labels now contains valid indices everywhere, gather works without error.\n",
    "        probs_for_true = softmax_probs.gather(dim=-1, index=adjusted_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Step 8: Compute token-level cross-entropy loss.\n",
    "        epsilon = 1e-12\n",
    "        token_losses = -torch.log(probs_for_true + epsilon)  # Shape: [batch, seq_len]\n",
    "        # Zero out the losses for tokens that should be ignored.\n",
    "        token_losses = token_losses * valid_mask.float()\n",
    "        \n",
    "        # Step 9: Average the loss only over valid tokens.\n",
    "        total_loss = token_losses.sum()\n",
    "        num_valid_tokens = valid_mask.sum().float()\n",
    "        avg_loss = total_loss / num_valid_tokens\n",
    "        \n",
    "        # Step 10: Compute perplexity as the exponential of the average loss.\n",
    "        perplexity = torch.exp(avg_loss).item()\n",
    "        \n",
    "        print(\"dfdfdfdfdfdddddd\")\n",
    "    else:\n",
    "        # Use the built-in loss from the model.\n",
    "        with torch.no_grad():\n",
    "            outputs = pp_model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "        perplexity = torch.exp(loss).item()\n",
    "\n",
    "    # Optionally, print both (or only the selected mode)\n",
    "    print(\"=============\")\n",
    "    print(\"Calculated perplexity (manually={0}): {1}\".format(manually, perplexity))\n",
    "    \n",
    "    print(\"=============\")\n",
    "    return perplexity\n",
    "\n",
    "candidate_2 = \"The big blue car drives quickly to the road\"\n",
    "perplexity = calculate_perplexity(candidate_2)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML - LLMs - Evaluation - BLEU - ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score:\n",
      "BLEU score for example 1: 0.46\n",
      "BLEU score for example 2: 0.51\n",
      "\n",
      "ROUGE Score:\n",
      "{'rouge1': {'precision': 1.0, 'recall': 0.6666666666666666, 'fmeasure': 0.8}, 'rouge2': {'precision': 0.6, 'recall': 0.375, 'fmeasure': 0.4615384615384615}, 'rougeL': {'precision': 1.0, 'recall': 0.6666666666666666, 'fmeasure': 0.8}}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from fractions import Fraction\n",
    "import math\n",
    "\n",
    "def calculate_rouge(references, candidate):\n",
    "    \"\"\"\n",
    "    Compute ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
    "    For each metric, compute scores for each reference and select the maximum F-measure.\n",
    "    \n",
    "    The parameter `references` can be either a single string or a list of strings.\n",
    "    \"\"\"\n",
    "    # Ensure references is a list.\n",
    "    if not isinstance(references, list):\n",
    "        references = [references]\n",
    "    \n",
    "    def tokenize(sentence):\n",
    "        return sentence.lower().split()\n",
    "    \n",
    "    def ngrams(tokens, n):\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    \n",
    "    def rouge_n(candidate_tokens, reference_tokens, n):\n",
    "        cand_ngrams = Counter(ngrams(candidate_tokens, n))\n",
    "        ref_ngrams = Counter(ngrams(reference_tokens, n))\n",
    "        overlap = sum(min(cand_ngrams[ngram], ref_ngrams[ngram]) for ngram in cand_ngrams)\n",
    "        precision = overlap / sum(cand_ngrams.values()) if sum(cand_ngrams.values()) > 0 else 0\n",
    "        recall = overlap / sum(ref_ngrams.values()) if sum(ref_ngrams.values()) > 0 else 0\n",
    "        if precision + recall == 0:\n",
    "            fscore = 0\n",
    "        else:\n",
    "            fscore = 2 * precision * recall / (precision + recall)\n",
    "        return precision, recall, fscore\n",
    "\n",
    "    # Use the provided code for LCS.\n",
    "    def lcs(text1, text2):\n",
    "        n1 = len(text1)\n",
    "        n2 = len(text2)\n",
    "        dp = [[0] * (n1 + 1) for _ in range(n2 + 1)]\n",
    "        for i in range(n2 - 1, -1, -1):\n",
    "            for j in range(n1 - 1, -1, -1):\n",
    "                if text2[i] != text1[j]:\n",
    "                    dp[i][j] = max(dp[i + 1][j], dp[i][j + 1])\n",
    "                else:\n",
    "                    dp[i][j] = 1 + dp[i + 1][j + 1]\n",
    "        return dp[0][0]\n",
    "\n",
    "    # Define ROUGE-L based on the LCS computed by our lcs function.\n",
    "    def rouge_l(candidate_tokens, reference_tokens):\n",
    "        lcs_length = lcs(candidate_tokens, reference_tokens)\n",
    "        precision = lcs_length / len(candidate_tokens) if candidate_tokens else 0\n",
    "        recall = lcs_length / len(reference_tokens) if reference_tokens else 0\n",
    "        if precision + recall == 0:\n",
    "            fscore = 0\n",
    "        else:\n",
    "            fscore = 2 * precision * recall / (precision + recall)\n",
    "        return precision, recall, fscore\n",
    "\n",
    "    # Tokenize candidate.\n",
    "    candidate_tokens = tokenize(candidate)\n",
    "    \n",
    "    # Compute ROUGE scores for each reference and select the best for each metric.\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for ref in references:\n",
    "        ref_tokens = tokenize(ref)\n",
    "        p1, r1, f1 = rouge_n(candidate_tokens, ref_tokens, 1)\n",
    "        p2, r2, f2 = rouge_n(candidate_tokens, ref_tokens, 2)\n",
    "        pL, rL, fL = rouge_l(candidate_tokens, ref_tokens)\n",
    "        rouge1_scores.append((p1, r1, f1))\n",
    "        rouge2_scores.append((p2, r2, f2))\n",
    "        rougeL_scores.append((pL, rL, fL))\n",
    "    \n",
    "    rouge1 = max(rouge1_scores, key=lambda x: x[2])\n",
    "    rouge2 = max(rouge2_scores, key=lambda x: x[2])\n",
    "    rougeL = max(rougeL_scores, key=lambda x: x[2])\n",
    "    \n",
    "    return {\n",
    "        \"rouge1\": {\"precision\": rouge1[0], \"recall\": rouge1[1], \"fmeasure\": rouge1[2]},\n",
    "        \"rouge2\": {\"precision\": rouge2[0], \"recall\": rouge2[1], \"fmeasure\": rouge2[2]},\n",
    "        \"rougeL\": {\"precision\": rougeL[0], \"recall\": rougeL[1], \"fmeasure\": rougeL[2]}\n",
    "    }\n",
    "\n",
    "def calculate_bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)):\n",
    "    \"\"\"\n",
    "    Compute a sentence-level BLEU score from scratch.\n",
    "    Uses the tutorial code with tokenization, modified precision with clipping,\n",
    "    and applies a brevity penalty.\n",
    "    \"\"\"\n",
    "    # If references is not a list, wrap it.\n",
    "    if not isinstance(references, list):\n",
    "        references = [references]\n",
    "    # Define helper functions.\n",
    "    def tokenize(sentence):\n",
    "        return sentence.lower().split()\n",
    "\n",
    "    def ngrams(tokens, n):\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "    def modified_precision(candidate_tokens, reference_tokens_list, n):\n",
    "        candidate_ngrams = Counter(ngrams(candidate_tokens, n))\n",
    "        max_ref_counts = Counter()\n",
    "        for ref in reference_tokens_list:\n",
    "            ref_ngrams = Counter(ngrams(ref, n))\n",
    "            # For each ngram in candidate, update with maximum count observed among references.\n",
    "            for ngram in candidate_ngrams:\n",
    "                max_ref_counts[ngram] = max(max_ref_counts[ngram], ref_ngrams[ngram])\n",
    "        clipped_counts = {ngram: min(count, max_ref_counts[ngram])\n",
    "                            for ngram, count in candidate_ngrams.items()}\n",
    "        numerator = sum(clipped_counts.values())\n",
    "        denominator = sum(candidate_ngrams.values())\n",
    "        if denominator == 0:\n",
    "            return 0\n",
    "        return Fraction(numerator, denominator)\n",
    "\n",
    "    def closest_reference_length(candidate_tokens, reference_tokens_list):\n",
    "        candidate_len = len(candidate_tokens)\n",
    "        ref_lens = [len(ref) for ref in reference_tokens_list]\n",
    "        return min(ref_lens, key=lambda ref_len: (abs(ref_len - candidate_len), ref_len))\n",
    "\n",
    "    def brevity_penalty(candidate_tokens, reference_tokens_list):\n",
    "        c_len = len(candidate_tokens)\n",
    "        closest_len = closest_reference_length(candidate_tokens, reference_tokens_list)\n",
    "        if c_len > closest_len:\n",
    "            return 1\n",
    "        else:\n",
    "            return math.exp(1 - closest_len / c_len) if c_len > 0 else 0\n",
    "\n",
    "    # Main BLEU computation.\n",
    "    candidate_tokens = tokenize(candidate)\n",
    "    reference_tokens_list = [tokenize(ref) for ref in references]\n",
    "    precisions = []\n",
    "    Hard_Smoothing = False\n",
    "    for i in range(len(weights)):\n",
    "        p = modified_precision(candidate_tokens, reference_tokens_list, i+1)\n",
    "        if Hard_Smoothing:\n",
    "            if p == 0:\n",
    "                p = Fraction(1, 10**9)  # smoothing: tiny value\n",
    "        precisions.append(float(p))\n",
    "    # Geometric mean of n-gram precisions.\n",
    "    if all(p == 0 for p in precisions):\n",
    "        return 0\n",
    "    \n",
    "    if Hard_Smoothing:\n",
    "        geo_mean = math.exp(sum(w * math.log(p) for w, p in zip(weights, precisions)))\n",
    "    else:\n",
    "        geo_mean = math.exp(sum(w * math.log(float(p)) for w, p in zip(weights, precisions) if p != 0))\n",
    "    bp = brevity_penalty(candidate_tokens, reference_tokens_list)\n",
    "    bleu = bp * geo_mean\n",
    "    return min(bleu, 1)\n",
    "\n",
    "candidate_1 = \"The quick brown dog jumps over the lazy fox\"\n",
    "references_1 = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"The fast brown fox leaps over the lazy dog\",\n",
    "]\n",
    "\n",
    "# Example 2\n",
    "candidate_2 = \"The big blue car drives quickly to the road\"\n",
    "references_2 = [\n",
    "    \"The small red car races quickly along the road\",\n",
    "    \"A small red car speeds rapidly down the avenue\",\n",
    "]\n",
    "bleu_score_scratch_1 = calculate_bleu(references_1,candidate_1, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "bleu_score_scratch_2 = calculate_bleu(references_2,candidate_2, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "print(\"BLEU score:\")\n",
    "print(f\"BLEU score for example 1: {bleu_score_scratch_1:.2f}\")\n",
    "print(f\"BLEU score for example 2: {bleu_score_scratch_2:.2f}\")\n",
    "\n",
    "reference = [\"The quick brown fox jumps over the lazy dog\"]\n",
    "candidate = \"The fox jumps over the dog\"\n",
    "print()\n",
    "print(\"ROUGE Score:\")\n",
    "print(calculate_rouge(reference, candidate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leetcode_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
